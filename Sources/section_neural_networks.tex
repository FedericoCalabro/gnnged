\documentclass[../Thesis.tex]{subfiles}
\begin{document}

\section{Neural Networks}
\label{sec:neural_networks}

A \emph{neural network} is a computational model that is inspired by the way biological neural networks in the human brain process information. It consists of interconnected units called neurons that work together to solve specific problems. The basic building block of a neural network is the perceptron, which computes a weighted sum of its inputs and passes the result through an activation function.

\subsection{Basic Structure of a Neural Network}

A simple neural network [\autoref{fig:basic-nn}] consists of three types of layers:

\begin{itemize}
    \item \textbf{Input Layer}: The layer that receives the input data.
    \item \textbf{Hidden Layers}: One or more intermediate layers that process the inputs received from the input layer.
    \item \textbf{Output Layer}: The layer that produces the final output.
\end{itemize}

The following figure illustrates a basic neural network with one hidden layer:
\subfile{../Tikz/tikz_monolayer_feedforward}

\subsection{Training Neural Networks}
Training a neural network involves adjusting the weights of the connections to minimize the error between the predicted output and the actual output. This is typically done using a method called \emph{backpropagation} along with an optimization algorithm like \emph{gradient descent}. The training process involves multiple iterations, where in each iteration, the network processes a batch of input data, calculates the output, computes the error, and updates the weights to reduce this error.

\subsubsection{Backpropagation}
Backpropagation [\autoref{fig:backpropagation}] is an algorithm used to compute the gradient of the loss function with respect to each weight by the chain rule, iterating backward from the last layer to the first layer. During the forward pass, the input data propagates through the network layer by layer until the final output is obtained. The error is then calculated using a loss function, which measures the difference between the predicted output and the actual output. In the backward pass, this error is propagated back through the network, allowing the algorithm to compute the gradients of the loss function with respect to each weight. These gradients indicate how much the loss would change with a small change in the weights, guiding the weight update process.
\subfile{../Tikz/tikz_backpropagation}

\subsubsection{Gradient Descent}
Gradient descent [\autoref{fig:grad-descent}] is an optimization algorithm used to minimize the loss function by iteratively moving towards the steepest descent, based on the computed gradients. At each iteration, the algorithm updates the weights by moving them in the opposite direction of the gradient of the loss function with respect to the weights. The size of these steps is determined by the learning rate, a hyperparameter that needs to be carefully chosen. A too large learning rate can cause the algorithm to overshoot the minimum, while a too small learning rate can make the convergence excessively slow.
\subfile{../Tikz/tikz_gradient_descent}
\subsection{Advanced Topics in Neural Networks}

\subsubsection{Deep Neural Networks}
A \emph{deep neural network} (DNN) [\autoref{fig:deep-nn}] is an artificial neural network with multiple hidden layers between the input and output layers. DNNs can model complex patterns and relationships in data, making them powerful tools for tasks such as image and speech recognition. The increased depth allows these networks to learn hierarchical representations of the data, capturing low-level features in the early layers and high-level features in the deeper layers. 
\subfile{../Tikz/tikz_multilayer_feedforward}

\subsubsection{Convolutional Neural Networks}
A \emph{convolutional neural network} (CNN) [\autoref{fig:cnn}] is a specialized type of neural network designed for processing structured grid data, like images. CNNs use convolutional layers that apply a series of filters to the input data to extract features. These filters slide over the input data, performing element-wise multiplications and summing the results, which helps in detecting patterns such as edges, textures, and shapes.
\subfile{../Tikz/tikz_convolutional_nn}

\subsubsection{Recurrent Neural Networks}
A \emph{recurrent neural network} (RNN) [\autoref{fig:rnn}] is a type of neural network that is well-suited for sequential data, such as time series or natural language. RNNs have connections that form directed cycles, allowing information to persist. This cyclic structure enables RNNs to maintain a hidden state that captures information from previous time steps, making them effective for tasks where the context is important, such as language modeling and machine translation.
\subfile{../Tikz/tikz_recurrent_nn}

\subsection{Graph Neural Networks and Attention Mechanisms}

\subsubsection{Graph Neural Networks}
A \emph{graph neural network} (GNN) is a type of neural network designed to handle graph-structured data. GNNs generalize neural networks to work directly on the graph domain by incorporating the graph's structure into the learning process. Nodes in a GNN aggregate feature information from their neighbors through multiple layers, enabling the network to capture the dependencies and relationships inherent in the graph. This message-passing mechanism [\autoref{fig:gnn}] allows GNNs to learn rich node representations and can be used for tasks such as node classification, link prediction, and graph classification. 
\subfile{../Tikz/tikz_message_passing}


\subsubsection{Attention Mechanisms}
Attention mechanisms are techniques used to enhance the performance of neural networks by dynamically focusing on relevant parts of the input data while processing. The most famous use of attention mechanisms is in the \emph{Transformer} model, which has revolutionized natural language processing tasks. The attention mechanism [\autoref{fig:attention}] computes a weighted sum of values, where the weights are derived from the compatibility of the query with the corresponding keys, allowing the model to selectively focus on important parts of the input. This selective focus helps improve the efficiency and accuracy of the model, especially in tasks involving sequences, such as machine translation and text summarization.  
The combination of GNNs and attention mechanisms has shown significant promise in various applications.
\subfile{../Tikz/tikz_attention_mechanism}


\end{document}