\documentclass[../Thesis.tex]{subfiles}
\begin{document}
	\chapter{Introduction}
	\label{sec:introduction}
	
	Graphs are fundamental data structures which are used to represent relationships between elements. A graph consists of vertices (or nodes) and edges (or links) which are the connections between any two vertices. This simple yet rich notation can model many realistic cases and therefore is a useful tool for studying many systems. For instance, social networks can be modeled as graphs where nodes are people, and edges are relationships between the people. This representation enables, among the others, the analysis of social processes, information diffusion, and the emergence of social communities. Graphs are also used to model and analyse biological systems, such as protein-protein interaction networks, brain networks, and ecological networks. City planners, and logistics scientists, similarly, use graphs to represent cities and transportation networks: cities are represented with nodes and the roads or the flights are represented through edges. Finally, graphs can be used to describe communication networks whereby devices are represented by nodes and connections by edges in order to determine the data flow, the strength of the network, and the best way to allocate resources. In short, graphs can represent a broad array of systems and they play a crucial role in analytical and optimization tasks.
	
	Networks are typically understood by studying their topological properties, such as connectivity, centrality, clustering coefficient, graph diameter and so on. These properties are useful to quantify certain aspects of a given graph, and have been used together in order to understand, in a qualitative way, how similar are different networks. The underlying idea is that two networks are similar if they share similar topological properties. For instance, networks with right-skewed degree distributions -- i.e. networks where a small number of nodes have a high number of connections while the vast majority of nodes has only a few connections -- have been shown to have similar behaviors in terms of network robustness or spreading dynamics\cite{albert2002statistical, pastor2001epidemic}. While this type of analysis turns out to be useful in order to define families of graphs, it is not very informative if one wants to measure the distance between two graphs.
	
	The Graph Edit Distance (GED) measures the number of edit operations -- such as insertions, deletions and substitutions of nodes and edges -- needed to transform one graph into another. This measure is extremely useful in many domains, such as in bio-informatics where it can be used to compare the shapes of molecules in order to find new drugs or study the evolution; in anti money-laundry scenarios where graphs of transactions are analysed and compared with a catalog of potentially fraudulent configurations; in forensic applications including fingerprint and handwriting recognition. In computer vision, GED is important in object recognition where the structural distance between the graphical models of different objects needs to be compared in order to differentiate between them. The GED is an extension of graph isomorphism: if two graphs have GED equal to zero, they are isomorphic.  
	
	Unfortunately, computing the GED between any two graphs requires solving an NP-Hard optimization problem where one has to search for the optimal edit path among all possible edit paths. This is typically achieved through an A$^*$ search algorithm. As such, the time required to compute the GED between two graphs becomes infeasible as soon as the graphs have more than 10/20 nodes. Several heuristics and approximation algorithms have been proposed in order to compute an approximation of the GED. Yet, most of these approximation algorithms have cubic computational cost. Therefore, researchers and practitioners are still working on ways to develop better approximation algorithms which can provide good results in a reasonable time. In particular, in recent years, the attention has turned towards neural networks.

	Neural networks are advanced tools for processing multi-dimensional data and form the basis of current machine learning. They mimic the brainâ€™s structure, with layers of interconnected  neurons that process inputs and produce outputs. Neural networks are trained with large datasets to fine-tune parameters based on the error between predicted and actual outputs. This training involves forward propagation, where input data is fed through the network to produce output, and back propagation, where errors adjust weights to enhance model precision. Neural networks have achieved high success rates in image and speech recognition, natural language processing, and graph data analysis, demonstrating their flexibility.
	
	Graph Neural Networks (GNNs), are a class of Neural Networks which are designed to operate on graph data type. These architectures try to take advantage of the graph structure by performing convolution operations over the nodes and edges of the graph. Through layers of convolutions, GNNs can capture both local and global properties of graphs. This makes them suitable for a number of applications such as node classification, link prediction and graph classification. GNNs work by enhancing the node representation at each step, with respect to its neighbors, thus capturing the relations and interdependencies in the graph. This is because the iterative process is useful in the learning of hierarchical representations that are essential in the understanding and analysis of graph-structured data, and thus improves the accuracy of the predictions in various applications. Given their ability to learn complex patterns and representations, in recent years a number of models based on GNNs have been proposed as methods to achieve fast and good approximations the GED. However, a critical review of the GNN-based models to approximate the GED, as well as a precise assessment of their strengths and weaknesses, to date are still missing. Thus, it is still unclear what the benefits and limitations of this line of research are.

\section{Objectives of the thesis}
 	 The current state-of-the-art models have been tested on a limited number of datasets which contain primarily small graphs. Moreover, currently no annotated dataset to train machine learning models to approximate the GED exists. Therefore, because computing the GED exactly is too expensive, the current way to build the labels for training models that approximate the GED is to approximate the GED between two graphs with 3 approximation algorithms and choosing the lower value \cite{simgnn__a_neural_network_approach_to_fast_graph_similarity_computation, computing_graph_edit_distance_via_neural_graph_matching}. While this is understandable, considering the hardness of computing the GED, using approximated values to train a model is asking to get an approximation of the approximation. The use of the best value among three heuristics might mitigate this problem, but it is not clear to which extent. Finally, while examining both the papers and the code-bases of these approaches, it emerged that the evaluation of such models is performed by retraining a model on each dataset, testing it only on the dataset's relative test set. This means that the current GNN-based methods to compute the GED might not generalize \emph{out-of-distribution} (OOD); that is, on graphs that are quite different from those on which they have been trained. Therefore, the present thesis aims at closing the aforementioned gaps in the following ways:

  \begin{enumerate}
      \item Determining the current state of the art by charting a timeline of the approaches that have been developed so far, describing the methods which seem to be more promising.
      \item Testing whether the current state of the art models generalize out-of-distribution by performing tests on synthetic datasets generated in a way that the GED between two graphs is known exactly.
      \item Providing an updated code-base, which implements the current state of the art models and that can serve as an initial platform to develop and test further approaches.
  \end{enumerate}


 \section{Contributions of this thesis}

	This thesis makes three contributions: \emph{1)} a literature review of the major approaches to approximate the Graph Edit Distance (GED) based on based on Graph Neural Networks (GNNs) including the first one, SimGNN \cite{simgnn__a_neural_network_approach_to_fast_graph_similarity_computation} and the last, most promising one in terms of performance, GedGNN \cite{computing_graph_edit_distance_via_neural_graph_matching}; \emph{2)} an updated code-base, with four major models implemented to run with Pytorch 2.2, which allows flexible experimentation; and \emph{3)} an independent assessment of the performance of the selected four models. In the following, the contributions of this thesis are briefly summarized.

    \begin{description}
        \item[Review of the state of the art:] The review of the state of the art has shown that GNN-based models to approximate the GED typically implement a Siamese network architecture which takes two graphs in input, computes their embedding and other operations and then estimates their distance. Most of the works, since the seminal model SimGNN \cite{simgnn__a_neural_network_approach_to_fast_graph_similarity_computation}, focused on improving performances by adding specialized layers such as the Graph Isomorphism Network (GIN) \cite{noah__neural_optimized_a*_search_algorithm_for_graph_edit_distance_computation}, or message passing neural networks \cite{learning_graph_edit_distance_by_graph_neural_networks}. Most of the approaches use some sort of attention mechanism \cite{simgnn__a_neural_network_approach_to_fast_graph_similarity_computation, noah__neural_optimized_a*_search_algorithm_for_graph_edit_distance_computation, exploring_attention_mechanism_for_graph_similarity_learning, graph_edit_distance_learning_via_different_attention, graph_graph_context_dependency_attention_for_graph_edit_distance}. Some approaches try to exploit the hierarchical structure of real world graphs \cite{graph_partitioning_and_graph_neural_network_based_hierarchical_graph_matching_for_graph_similarity_computation, h2mn__graph_similarity_learning_with_hierarchical_hypergraph_matching_networks, multilevel_graph_matching_networks_for_deep_graph_similarity_learning}. Other approaches complement the aforementioned ones with graph matching mechanisms \cite{more_interpretable_graph_similarity_computation_via_maximum_common_subgraph_inference, multilevel_graph_matching_networks_for_deep_graph_similarity_learning, h2mn__graph_similarity_learning_with_hierarchical_hypergraph_matching_networks, computing_graph_edit_distance_via_neural_graph_matching}. All these approaches share the fact that they build a Siamese network and then add layers and operations to obtain better graph embedding, in order to better predict the GED.
        Few approaches differs in that they combine traditional algorithms with GNNs in an attempt to achieve more efficient computations and better interpretability. In this category, there are approaches which combine the A$^*$ search algorithm with GNNs \cite{combinatorial_learning_of_graph_edit_distance_via_dynamic_embedding, noah__neural_optimized_a*_search_algorithm_for_graph_edit_distance_computation, mata_combining_learnable_node_matching_with_a*_algorithm_for_approximate_graph_edit_distance}.
        \item[Code-base:] As discussed in section \ref{sec:issues-codebase}, the original code-base associated with the GedGNN paper\cite{computing_graph_edit_distance_via_neural_graph_matching}, which is the current state of the art in terms of performance, had some issues. In particular, \emph{1)} it did not run on the last version of Pytorch; \emph{2)} had performance issues regarding GPU computation; \emph{3)} it was working only with small graphs (with at most 10 nodes) and did not allow flexibility on the dataset format; and \emph{4)} it was not ready to work with external datasets. The code-base implemented with this thesis solved these issues and can be used as a platform to be extended with other models and datasets.

        \item[Independent assessment of the models' performance:] This thesis presents an independent assessment of the models' performance in terms of both \emph{1)} reproducing the experiments from \cite{computing_graph_edit_distance_via_neural_graph_matching}, and \emph{2)} testing whether the models generalise out-of-distribution. It was possible to reproduce the performance of the GedGNN model \cite{computing_graph_edit_distance_via_neural_graph_matching}, which confirms to be the one with the best performance among the ones used in the paper; namely SimGNN \cite{simgnn__a_neural_network_approach_to_fast_graph_similarity_computation}, GPN \cite{noah__neural_optimized_a*_search_algorithm_for_graph_edit_distance_computation}, and TaGSim \cite{TaGSim_type_aware_graph_similarity_learning_and_computation}. However, by synthetically generating a dataset of couples of graphs at known GED, it was found that none of these four models generalizes out-of-distribution. The models exhibit good performance only when tested on the relative test set of the dataset on which they have been trained.
    \end{description}

In sum, despite the great deal of research in the field of GNN-based GED computation, current models are still far from meeting the promises of faster and more accurate GED computation.
In particular, compared with classical algorithmic approximation which can perform decently on any couple of graphs, the methods considered in this thesis can perform well only on graphs which are similar to those on which they have been trained.
As such, future research in this field should focus on improving the training data for these models and solving their lack of out-of-distribution generalization.

\end{document}
