{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from texttable import Texttable\n",
    "\n",
    "def tab_printer(args):\n",
    "    \"\"\"\n",
    "    Function to print the logs in a nice tabular format.\n",
    "    :param args: Parameters used for the model.\n",
    "    \"\"\"\n",
    "    args = vars(args)\n",
    "    keys = sorted(args.keys())\n",
    "    t = Texttable()\n",
    "    t.add_rows([[\"Parameter\", \"Value\"]])\n",
    "    t.add_rows([[k.replace(\"_\", \" \").capitalize(), args[k]] for k in keys])\n",
    "    print(t.draw())\n",
    "\n",
    "def denormalize_sim_score(g1, g2, sim_score):\n",
    "    \"\"\"\n",
    "    Converts normalized similarity into ged.\n",
    "    \"\"\"\n",
    "    return denormalize_ged(g1, g2, -math.log(sim_score, math.e))\n",
    "\n",
    "\n",
    "def denormalize_ged(g1, g2, nged):\n",
    "    \"\"\"\n",
    "    Converts normalized ged into ged.\n",
    "    \"\"\"\n",
    "    return round(nged * (g1.num_nodes + g2.num_nodes) / 2) if nged != np.inf else np.inf\n",
    "\n",
    "def to_directed(edge_index):\n",
    "    row, col = edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "    return torch.stack([row, col], dim=0)\n",
    "\n",
    "def calculate_ranking_correlation(rank_corr_function, prediction, target):\n",
    "    \"\"\"\n",
    "    Calculating specific ranking correlation for predicted values.\n",
    "    :param rank_corr_function: Ranking correlation function.\n",
    "    :param prediction: Vector of predicted values.\n",
    "    :param target: Vector of ground-truth values.\n",
    "    :return ranking: Ranking correlation value.\n",
    "    \"\"\"\n",
    "    temp = prediction.argsort()\n",
    "    r_prediction = np.empty_like(temp)\n",
    "    r_prediction[temp] = np.arange(len(prediction))\n",
    "\n",
    "    temp = target.argsort()\n",
    "    r_target = np.empty_like(temp)\n",
    "    r_target[temp] = np.arange(len(target))\n",
    "\n",
    "    return rank_corr_function(r_prediction, r_target).correlation\n",
    "\n",
    "def calculate_prec_at_k(k, prediction, target):\n",
    "    \"\"\"\n",
    "    Calculating precision at k.\n",
    "    \"\"\"\n",
    "    best_k_pred = prediction.argsort()[:k]\n",
    "    best_k_target = target.argsort()[:k]\n",
    "\n",
    "    return len(set(best_k_pred).intersection(set(best_k_target))) / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "#TODO: Added 'args' parameter in order to be able to parse arguments using a pynotebook (it was CLI only)\n",
    "def parameter_parser(args : list):\n",
    "    \"\"\"\n",
    "    A method to parse up command line parameters.\n",
    "    The default hyperparameters give a high performance model without grid search.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Run SimGNN.\")\n",
    "\n",
    "    parser.add_argument(\"--dataset\",\n",
    "                        nargs=\"?\",\n",
    "                        default=\"AIDS700nef\",  # AIDS700nef LINUX IMDBMulti\n",
    "                        help=\"Dataset name. Default is AIDS700nef\")\n",
    "    parser.add_argument(\"--epochs\",\n",
    "                        type=int,\n",
    "                        default=5,\n",
    "\t                help=\"Number of training epochs. Default is 5.\")\n",
    "\n",
    "    parser.add_argument(\"--filters-1\",\n",
    "                        type=int,\n",
    "                        default=128,\n",
    "\t                help=\"Filters (neurons) in 1st convolution. Default is 128.\")\n",
    "\n",
    "    parser.add_argument(\"--filters-2\",\n",
    "                        type=int,\n",
    "                        default=64,\n",
    "\t                help=\"Filters (neurons) in 2nd convolution. Default is 64.\")\n",
    "\n",
    "    parser.add_argument(\"--filters-3\",\n",
    "                        type=int,\n",
    "                        default=32,\n",
    "\t                help=\"Filters (neurons) in 3rd convolution. Default is 32.\")\n",
    "\n",
    "    parser.add_argument(\"--tensor-neurons\",\n",
    "                        type=int,\n",
    "                        default=16,\n",
    "\t                help=\"Neurons in tensor network layer. Default is 16.\")\n",
    "\n",
    "    parser.add_argument(\"--bottle-neck-neurons\",\n",
    "                        type=int,\n",
    "                        default=16,\n",
    "\t                help=\"Bottle neck layer neurons. Default is 16.\")\n",
    "\n",
    "    parser.add_argument(\"--batch-size\",\n",
    "                        type=int,\n",
    "                        default=128,\n",
    "\t                help=\"Number of graph pairs per batch. Default is 128.\")\n",
    "\n",
    "    parser.add_argument(\"--bins\",\n",
    "                        type=int,\n",
    "                        default=16,\n",
    "\t                help=\"Similarity score bins. Default is 16.\")\n",
    "\n",
    "    parser.add_argument(\"--dropout\",\n",
    "                        type=float,\n",
    "                        default=0.5,\n",
    "\t                help=\"Dropout probability. Default is 0.5.\")\n",
    "\n",
    "    parser.add_argument(\"--learning-rate\",\n",
    "                        type=float,\n",
    "                        default=0.001,\n",
    "\t                help=\"Learning rate. Default is 0.001.\")\n",
    "\n",
    "    parser.add_argument(\"--weight-decay\",\n",
    "                        type=float,\n",
    "                        default=5*10**-4,\n",
    "\t                help=\"Adam weight decay. Default is 5*10^-4.\")\n",
    "\n",
    "    parser.add_argument(\"--histogram\",\n",
    "                        dest=\"histogram\",\n",
    "                        action=\"store_true\")\n",
    "\n",
    "    parser.set_defaults(histogram=False)\n",
    "\n",
    "    parser.add_argument(\"--save-path\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"Where to save the trained model\")\n",
    "\n",
    "    parser.add_argument(\"--load-path\",\n",
    "                        type=str,\n",
    "                        default=None,\n",
    "                        help=\"Load a pretrained model\")\n",
    "\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "\n",
    "#TODO: Replace with torch.scatter\n",
    "def scatter_(name, src, index, dim=0, dim_size=None):\n",
    "    assert name in ['add', 'mean', 'min', 'max']\n",
    "\n",
    "    op = getattr(torch_scatter, 'scatter_{}'.format(name))\n",
    "    out = op(src, index, dim, None, dim_size)\n",
    "    out = out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "    if name == 'max':\n",
    "        out[out < -10000] = 0\n",
    "    elif name == 'min':\n",
    "        out[out > 10000] = 0\n",
    "\n",
    "    return out\n",
    "\n",
    "class AttentionModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN Attention Module to make a pass on graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.setup_weights()\n",
    "        self.init_parameters()\n",
    "\n",
    "    def setup_weights(self):\n",
    "        \"\"\"\n",
    "        Defining weights.\n",
    "        \"\"\"\n",
    "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.args.filters_3, self.args.filters_3))\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializing weights.\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "\n",
    "    def forward(self, x, batch, size=None):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a graph level representation.\n",
    "        :param x: Result of the GNN.\n",
    "        :param batch: Batch vector, which assigns each node to a specific example\n",
    "        :return representation: A graph level representation matrix. \n",
    "        \"\"\"\n",
    "        size = batch[-1].item() + 1 if size is None else size\n",
    "        mean = scatter_('mean', x, batch, dim_size=size)\n",
    "        transformed_global = torch.tanh(torch.mm(mean, self.weight_matrix))\n",
    "\n",
    "        coefs = torch.sigmoid((x * transformed_global[batch]).sum(dim=1))\n",
    "        weighted = coefs.unsqueeze(-1) * x\n",
    "\n",
    "        return scatter_('add', weighted, batch, dim_size=size)\n",
    "\n",
    "class TensorNetworkModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN Tensor Network module to calculate similarity vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(TensorNetworkModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.setup_weights()\n",
    "        self.init_parameters()\n",
    "\n",
    "    def setup_weights(self):\n",
    "        \"\"\"\n",
    "        Defining weights.\n",
    "        \"\"\"\n",
    "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.args.filters_3, self.args.filters_3, self.args.tensor_neurons))\n",
    "        self.weight_matrix_block = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons, 2*self.args.filters_3))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons, 1))\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializing weights.\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix_block)\n",
    "        torch.nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, embedding_1, embedding_2):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a similarity vector.\n",
    "        :param embedding_1: Result of the 1st embedding after attention.\n",
    "        :param embedding_2: Result of the 2nd embedding after attention.\n",
    "        :return scores: A similarity score vector.\n",
    "        \"\"\"\n",
    "        batch_size = len(embedding_1)\n",
    "        scoring = torch.matmul(embedding_1, self.weight_matrix.view(self.args.filters_3, -1))\n",
    "        scoring = scoring.view(batch_size, self.args.filters_3, -1).permute([0, 2, 1])\n",
    "        scoring = torch.matmul(scoring, embedding_2.view(batch_size, self.args.filters_3, 1)).view(batch_size, -1)\n",
    "        combined_representation = torch.cat((embedding_1, embedding_2), 1)\n",
    "        block_scoring = torch.t(torch.mm(self.weight_matrix_block, torch.t(combined_representation)))\n",
    "        scores = torch.nn.functional.relu(scoring + block_scoring + self.bias.view(-1))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "class SimGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n",
    "    https://arxiv.org/abs/1808.05689\n",
    "    \"\"\"\n",
    "    def __init__(self, args, number_of_node_labels, number_of_edge_labels):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :param number_of_labels: Number of node labels.\n",
    "        \"\"\"\n",
    "        super(SimGNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.number_node_labels = number_of_node_labels\n",
    "        # self.number_edge_labels = number_of_edge_labels\n",
    "        self.setup_layers()    \n",
    "        \n",
    "    def calculate_bottleneck_features(self):\n",
    "        \"\"\"\n",
    "        Deciding the shape of the bottleneck layer.\n",
    "        \"\"\"\n",
    "        if self.args.histogram == True:\n",
    "            self.feature_count = self.args.tensor_neurons + self.args.bins\n",
    "        else:\n",
    "            self.feature_count = self.args.tensor_neurons\n",
    "            \n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layers.\n",
    "        \"\"\"\n",
    "        self.calculate_bottleneck_features()\n",
    "        self.convolution_1 = GCNConv(self.number_node_labels, self.args.filters_1)\n",
    "        self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)\n",
    "        self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)\n",
    "        self.attention = AttentionModule(self.args)\n",
    "        self.tensor_network = TensorNetworkModule(self.args)\n",
    "        self.fully_connected_first = torch.nn.Linear(self.feature_count, self.args.bottle_neck_neurons)\n",
    "        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons, 1)\n",
    "        \n",
    "    def calculate_histogram(self, abstract_features_1, abstract_features_2, batch_1, batch_2):\n",
    "        \"\"\"\n",
    "        Calculate histogram from similarity matrix.\n",
    "        :param abstract_features_1: Feature matrix for target graphs.\n",
    "        :param abstract_features_2: Feature matrix for source graphs.\n",
    "        :param batch_1: Batch vector for source graphs, which assigns each node to a specific example\n",
    "        :param batch_1: Batch vector for target graphs, which assigns each node to a specific example\n",
    "        :return hist: Histsogram of similarity scores.\n",
    "        \"\"\"\n",
    "        abstract_features_1, mask_1 = to_dense_batch(abstract_features_1, batch_1)\n",
    "        abstract_features_2, mask_2 = to_dense_batch(abstract_features_2, batch_2)\n",
    "\n",
    "        B1, N1, _ = abstract_features_1.size()\n",
    "        B2, N2, _ = abstract_features_2.size()\n",
    "\n",
    "        mask_1 = mask_1.view(B1, N1)\n",
    "        mask_2 = mask_2.view(B2, N2)\n",
    "        num_nodes = torch.max(mask_1.sum(dim=1), mask_2.sum(dim=1))\n",
    "\n",
    "        scores = torch.matmul(abstract_features_1, abstract_features_2.permute([0, 2, 1])).detach()\n",
    "\n",
    "        hist_list = []\n",
    "        for i, mat in enumerate(scores):\n",
    "            mat = torch.sigmoid(mat[:num_nodes[i], :num_nodes[i]]).view(-1)\n",
    "            hist = torch.histc(mat, bins=self.args.bins)\n",
    "            hist = hist / torch.sum(hist)\n",
    "            hist = hist.view(1, -1)\n",
    "            hist_list.append(hist)\n",
    "\n",
    "        return torch.stack(hist_list).view(-1, self.args.bins)\n",
    "    \n",
    "    def convolutional_pass(self, edge_index, features):\n",
    "        \"\"\"\n",
    "        Making convolutional pass.\n",
    "        :param edge_index: Edge indices.\n",
    "        :param features: Feature matrix.\n",
    "        :return features: Absstract feature matrix.\n",
    "        \"\"\"\n",
    "        features = self.convolution_1(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features,\n",
    "                                               p=self.args.dropout,\n",
    "                                               training=self.training)\n",
    "\n",
    "        features = self.convolution_2(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features,\n",
    "                                               p=self.args.dropout,\n",
    "                                               training=self.training)\n",
    "\n",
    "        features = self.convolution_3(features, edge_index)\n",
    "        return features\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass with graphs.\n",
    "        :param data: Data dictionary.\n",
    "        :return score: Similarity score.\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph attribute initiallizations\n",
    "        g1 = data[\"g1\"]\n",
    "        g2 = data[\"g2\"]\n",
    "\n",
    "        pooled_features_1, abstract_features_1, batch_1 = self.get_embedding(g1)\n",
    "        pooled_features_2, abstract_features_2, batch_2 = self.get_embedding(g2)\n",
    "\n",
    "        # Output scores' vector\n",
    "        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n",
    "\n",
    "        if self.args.histogram:\n",
    "            hist = self.calculate_histogram(abstract_features_1, abstract_features_2, batch_1, batch_2)\n",
    "            scores = torch.cat((scores, hist), dim=1)\n",
    "\n",
    "        # Final connected layers\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_first(scores))\n",
    "        score = torch.sigmoid(self.scoring_layer(scores)).view(-1)\n",
    "\n",
    "        return score\n",
    "    \n",
    "    def get_embedding(self, g):\n",
    "        \"\"\"\n",
    "        This method implements the first 2 steps of the pipeline for a given graph. It used to be directly implemented\n",
    "        in the forward() method. It returns intermediate data too, because they are needed from \"Strategy 2\" module.\n",
    "        :param g: The graph we want to take the embedding for.\n",
    "        :return: The embedding of the given graph g, abstract_features(node embeddings) and batch.\n",
    "        \"\"\"\n",
    "        # Graph attribute initiallizations\n",
    "        edge_index = g.edge_index\n",
    "\n",
    "        # edge_attr = g.edge_attr\n",
    "        features = g.x\n",
    "        batch = g.batch if hasattr(g, 'batch') else torch.tensor((), dtype=torch.long).new_zeros(g.num_nodes)\n",
    "\n",
    "        # Node embeddings\n",
    "        abstract_features = self.convolutional_pass(edge_index, features)\n",
    "\n",
    "        # Graph embeddings\n",
    "        graph_embedding = self.attention(abstract_features, batch)\n",
    "\n",
    "        return graph_embedding, abstract_features, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEDDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_url(url, folder, log=True):\n",
    "    r\"\"\"Downloads the content of an URL to a specific folder.\n",
    "\n",
    "    Args:\n",
    "        url (string): The url.\n",
    "        folder (string): The folder.\n",
    "        log (bool, optional): If :obj:`False`, will not print anything to the\n",
    "            console. (default: :obj:`True`)\n",
    "    \"\"\"\n",
    "    filename = url.rpartition('/')[2]\n",
    "    path = osp.join(folder, filename)\n",
    "\n",
    "    if osp.exists(path):  # pragma: no cover\n",
    "        if log:\n",
    "            print('Using exist file', filename)\n",
    "        return path\n",
    "\n",
    "    if log:\n",
    "        print('Downloading', url)\n",
    "        \n",
    "    # Create Folder if not exist\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        \n",
    "    data = urllib.request.urlopen(url)\n",
    "\n",
    "    path = path.replace('?', '')\n",
    "    with open(path, 'wb', ) as f:\n",
    "        f.write(data.read())\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from torch_geometric.data import (InMemoryDataset, Data, extract_zip, extract_tar)\n",
    "from torch_geometric.utils import to_undirected\n",
    "\n",
    "\n",
    "class GEDDataset(InMemoryDataset):\n",
    "\n",
    "    url = 'https://drive.google.com/uc?export=download&id={}'\n",
    "\n",
    "    datasets = {\n",
    "        'AIDS700nef': {\n",
    "            'id': '10czBPJDEzEDI2tq7Z7mkBjLhj55F-a2z',\n",
    "            'extract': extract_zip,\n",
    "            'pickle': '1OpV4bCHjBkdpqI6H5Mg0-BqlA2ee2eBW',\n",
    "        },\n",
    "        'LINUX': {\n",
    "            'id': '1nw0RRVgyLpit4V4XFQyDy0pI6wUEXSOI',\n",
    "            'extract': extract_tar,\n",
    "            'pickle': '14FDm3NSnrBvB7eNpLeGy5Bz6FjuCSF5v',\n",
    "        },\n",
    "        'ALKANE': {\n",
    "            'id': '1-LmxaWW3KulLh00YqscVEflbqr0g4cXt',\n",
    "            'extract': extract_tar,\n",
    "            'pickle': '15BpvMuHx77-yUGYgM27_sQett02HQNYu',\n",
    "        },\n",
    "        'IMDBMulti': {\n",
    "            'id': '12QxZ7EhYA7pJiF4cO-HuE8szhSOWcfST',\n",
    "            'extract': extract_zip,\n",
    "            'pickle': '1wy9VbZvZodkixxVIOuRllC-Lp-0zdoYZ',\n",
    "        },\n",
    "    }\n",
    "\n",
    "    types = [\n",
    "        'O', 'S', 'C', 'N', 'Cl', 'Br', 'B', 'Si', 'Hg', 'I', 'Bi', 'P', 'F',\n",
    "        'Cu', 'Ho', 'Pd', 'Ru', 'Pt', 'Sn', 'Li', 'Ga', 'Tb', 'As', 'Co', 'Pb',\n",
    "        'Sb', 'Se', 'Ni', 'Te'\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 name,\n",
    "                 train=True,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None):\n",
    "        self.name = name\n",
    "        assert self.name in self.datasets.keys()\n",
    "        super(GEDDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        path = self.processed_paths[0] if train else self.processed_paths[1]\n",
    "        self.data, self.slices = torch.load(path)\n",
    "        path = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))\n",
    "        self.ged = torch.load(path)\n",
    "        path = osp.join(self.processed_dir, '{}_norm_ged.pt'.format(self.name))\n",
    "        self.norm_ged = torch.load(path)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [osp.join(self.name, s) for s in ['train', 'test']]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['{}_{}.pt'.format(self.name, s) for s in ['training', 'test']]\n",
    "\n",
    "    def download(self):\n",
    "        name = self.datasets[self.name]['id']\n",
    "        path = download_url(self.url.format(name), self.raw_dir)\n",
    "        self.datasets[self.name]['extract'](path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "        name = self.datasets[self.name]['pickle']\n",
    "        path = download_url(self.url.format(name), self.raw_dir)\n",
    "        os.rename(path, osp.join(self.raw_dir, self.name, 'ged.pickle'))\n",
    "\n",
    "    def process(self):\n",
    "        ids, Ns = [], []\n",
    "        for r_path, p_path in zip(self.raw_paths, self.processed_paths):\n",
    "            names = glob.glob(osp.join(r_path, '*.gexf'))\n",
    "            ids.append(sorted([int(osp.basename(i)[:-5]) for i in names]))\n",
    "\n",
    "            data_list = []\n",
    "            for i, idx in enumerate(ids[-1]):\n",
    "                i = i if len(ids) == 1 else i + len(ids[0])\n",
    "                G = nx.read_gexf(osp.join(r_path, '{}.gexf'.format(idx)))\n",
    "                mapping = {name: j for j, name in enumerate(G.nodes())}\n",
    "                G = nx.relabel_nodes(G, mapping)\n",
    "                Ns.append(G.number_of_nodes())\n",
    "                edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "                if edge_index.numel() == 0:\n",
    "                    edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "                edge_index = to_undirected(edge_index, num_nodes=Ns[-1])\n",
    "\n",
    "                data = Data(edge_index=edge_index, i=i)\n",
    "                data.num_nodes = Ns[-1]\n",
    "\n",
    "                if self.name == 'AIDS700nef':\n",
    "                    x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "                    for node, info in G.nodes(data=True):\n",
    "                        x[int(node)] = self.types.index(info['type'])\n",
    "                    data.x = F.one_hot(\n",
    "                        x, num_classes=len(self.types)).to(torch.float)\n",
    "\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "                data_list.append(data)\n",
    "            torch.save(self.collate(data_list), p_path)\n",
    "\n",
    "        assoc = {idx: i for i, idx in enumerate(ids[0])}\n",
    "        assoc.update({idx: i + len(ids[0]) for i, idx in enumerate(ids[1])})\n",
    "\n",
    "        path = osp.join(self.raw_dir, self.name, 'ged.pickle')\n",
    "        mat = torch.full((len(assoc), len(assoc)), float('inf'))\n",
    "        with open(path, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "            xs, ys, gs = [], [], []\n",
    "            for (x, y), g in obj.items():\n",
    "                xs += [assoc[x]]\n",
    "                ys += [assoc[y]]\n",
    "                gs += [g]\n",
    "            x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs, dtype=torch.float)\n",
    "            mat[x, y], mat[y, x] = g, g\n",
    "\n",
    "        path = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))\n",
    "        torch.save(mat, path)\n",
    "\n",
    "        N = torch.tensor(Ns, dtype=torch.float)\n",
    "        norm_mat = mat / (0.5 * (N.view(-1, 1) + N.view(1, -1)))\n",
    "\n",
    "        path = osp.join(self.processed_dir, '{}_norm_ged.pt'.format(self.name))\n",
    "        torch.save(norm_mat, path)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: in the future\n",
    "from torch_geometric.data import (InMemoryDataset)\n",
    "\n",
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 name,\n",
    "                 train=True,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None):\n",
    "        self.name = name\n",
    "        super(CustomDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        path = self.processed_paths[0] if train else self.processed_paths[1]\n",
    "        print(path)\n",
    "        # self.data, self.slices = torch.load(path)\n",
    "        # path = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))\n",
    "        # self.ged = torch.load(path)\n",
    "        # path = osp.join(self.processed_dir, '{}_norm_ged.pt'.format(self.name))\n",
    "        # self.norm_ged = torch.load(path)\n",
    "        \n",
    "    def process(self):\n",
    "        ids, Ns = [], []\n",
    "        for r_path, p_path in zip(self.raw_paths, self.processed_paths):\n",
    "            names = glob.glob(osp.join(r_path, '*.json'))\n",
    "            ids.append(sorted([int(i.split(os.sep)[-1][:-5]) for i in names]))\n",
    "\n",
    "            print(ids)\n",
    "            data_list = []\n",
    "            for i, idx in enumerate(ids[-1]):\n",
    "                i = i if len(ids) == 1 else i + len(ids[0])\n",
    "                G = nx.read_gexf(osp.join(r_path, '{}.gexf'.format(idx)))\n",
    "        #         mapping = {name: j for j, name in enumerate(G.nodes())}\n",
    "        #         G = nx.relabel_nodes(G, mapping)\n",
    "        #         Ns.append(G.number_of_nodes())\n",
    "        #         edge_index = torch.tensor(list(G.edges)).t().contiguous()\n",
    "        #         if edge_index.numel() == 0:\n",
    "        #             edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        #         edge_index = to_undirected(edge_index, num_nodes=Ns[-1])\n",
    "\n",
    "        #         data = Data(edge_index=edge_index, i=i)\n",
    "        #         data.num_nodes = Ns[-1]\n",
    "\n",
    "        #         if self.name == 'AIDS700nef':\n",
    "        #             x = torch.zeros(data.num_nodes, dtype=torch.long)\n",
    "        #             for node, info in G.nodes(data=True):\n",
    "        #                 x[int(node)] = self.types.index(info['type'])\n",
    "        #             data.x = F.one_hot(\n",
    "        #                 x, num_classes=len(self.types)).to(torch.float)\n",
    "\n",
    "        #         if self.pre_filter is not None and not self.pre_filter(data):\n",
    "        #             continue\n",
    "        #         if self.pre_transform is not None:\n",
    "        #             data = self.pre_transform(data)\n",
    "        #         data_list.append(data)\n",
    "        #     torch.save(self.collate(data_list), p_path)\n",
    "\n",
    "        # assoc = {idx: i for i, idx in enumerate(ids[0])}\n",
    "        # assoc.update({idx: i + len(ids[0]) for i, idx in enumerate(ids[1])})\n",
    "\n",
    "        # path = osp.join(self.raw_dir, self.name, 'ged.pickle')\n",
    "        # mat = torch.full((len(assoc), len(assoc)), float('inf'))\n",
    "        # with open(path, 'rb') as f:\n",
    "        #     obj = pickle.load(f)\n",
    "        #     xs, ys, gs = [], [], []\n",
    "        #     for (x, y), g in obj.items():\n",
    "        #         xs += [assoc[x]]\n",
    "        #         ys += [assoc[y]]\n",
    "        #         gs += [g]\n",
    "        #     x, y, g = torch.tensor(xs), torch.tensor(ys), torch.tensor(gs, dtype=torch.float)\n",
    "        #     mat[x, y], mat[y, x] = g, g\n",
    "\n",
    "        # path = osp.join(self.processed_dir, '{}_ged.pt'.format(self.name))\n",
    "        # torch.save(mat, path)\n",
    "\n",
    "        # N = torch.tensor(Ns, dtype=torch.float)\n",
    "        # norm_mat = mat / (0.5 * (N.view(-1, 1) + N.view(1, -1)))\n",
    "\n",
    "        # path = osp.join(self.processed_dir, '{}_norm_ged.pt'.format(self.name))\n",
    "        # torch.save(norm_mat, path)\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [osp.join(self.name, s) for s in ['train', 'test']]\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['{}_{}.pt'.format(self.name, s) for s in ['training', 'test']]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.name, len(self))\n",
    "    \n",
    "# TRAIN = CustomDataset(\"datasets/extrasmall\", \"extrasmall\", train=True)\n",
    "# TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.transforms import OneHotDegree\n",
    "from tqdm import tqdm, trange\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "class SimGNNTrainer(object):\n",
    "    \"\"\"\n",
    "    SimGNN model trainer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.process_dataset()\n",
    "        self.model, self.optimizer = self.setup_model()\n",
    "        \n",
    "    def setup_model(self):\n",
    "        \"\"\"\n",
    "        Creating a SimGNN.\n",
    "        \"\"\"\n",
    "        model = SimGNN(self.args, self.number_of_node_labels, self.number_of_edge_labels).to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.args.learning_rate, weight_decay=self.args.weight_decay)\n",
    "\n",
    "        return model, optimizer\n",
    "    \n",
    "    def process_dataset(self):\n",
    "        \"\"\"\n",
    "        Downloading and processing dataset.\n",
    "        \"\"\"\n",
    "        print(\"\\nPreparing dataset.\\n\")\n",
    "        self.training_graphs = GEDDataset(f'datasets/{self.args.dataset}', self.args.dataset, train=True)\n",
    "        self.testing_graphs = GEDDataset(f'datasets/{self.args.dataset}', self.args.dataset, train=False)\n",
    "        \n",
    "        self.ged_matrix = self.training_graphs.ged\n",
    "        self.nged_matrix = self.training_graphs.norm_ged\n",
    "\n",
    "        if self.training_graphs[0].x is None:\n",
    "            max_degree = 0\n",
    "            for g in self.training_graphs + self.testing_graphs:\n",
    "                if g.edge_index.size(1) > 0:\n",
    "                    max_degree = max(max_degree, int(degree(g.edge_index[0]).max().item()))\n",
    "            one_hot_degree = OneHotDegree(max_degree, cat=False)\n",
    "            self.training_graphs.transform = one_hot_degree\n",
    "            self.testing_graphs.transform = one_hot_degree\n",
    "\n",
    "        self.number_of_node_labels = self.training_graphs.num_features\n",
    "        self.number_of_edge_labels = self.training_graphs.num_edge_features\n",
    "\n",
    "    def create_train_batches(self, train_set):\n",
    "            \"\"\"\n",
    "            Creating suffled batches from the training graph list.\n",
    "            :return batches: Zipped loaders as list.\n",
    "            \"\"\"\n",
    "            source_loader = DataLoader(dataset=train_set.shuffle(), batch_size=self.args.batch_size)\n",
    "            target_loader = DataLoader(dataset=train_set.shuffle(), batch_size=self.args.batch_size)\n",
    "\n",
    "            return list(zip(source_loader, target_loader))\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Getting ground truth GED for graph pairs and grouping as data into dictionary.\n",
    "        :param data: Graph pair - tuple list for 1 target graph with any number of source graphs.\n",
    "        :return new_data: Dictionary with data. Data contain the source graphs as g1, the target graph as g2 and the\n",
    "        normalized ged (exponentiated) for each pair as target.\n",
    "        \"\"\"\n",
    "        new_data = dict()\n",
    "\n",
    "        new_data[\"g1\"] = data[0].to(device)\n",
    "        new_data[\"g2\"] = data[1].to(device)\n",
    "\n",
    "        # for each g1 and g2, access the lists with the graphs' index 'i', and retrieve the respective ged value.\n",
    "        normalized_ged = self.nged_matrix[data[0][\"i\"].reshape(-1).tolist(), data[1][\"i\"].reshape(-1).tolist()].tolist()\n",
    "        new_data[\"target\"] = torch.from_numpy(np.exp([(-el) for el in normalized_ged])).view(-1).float().to(device)\n",
    "\n",
    "        return new_data\n",
    "\n",
    "    def process_batch(self, data):\n",
    "        \"\"\"\n",
    "        Performs the forward pass with a batch of data.\n",
    "        :param data: Data that is essentially pairs of batches, for source and target graphs.\n",
    "        :return loss: Loss on the data.\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        data = self.transform(data)\n",
    "        prediction = self.model(data)\n",
    "        loss = torch.nn.functional.mse_loss(prediction, data[\"target\"], reduction='sum')\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def fit(self):\n",
    "            \"\"\"\n",
    "            Training a model.\n",
    "            \"\"\"\n",
    "            self.model.train()\n",
    "\n",
    "            train_set = self.training_graphs\n",
    "            e = self.args.epochs\n",
    "            epochs = trange(e, leave=None, desc=\"Epoch\", position=0)\n",
    "\n",
    "            loss_list = []\n",
    "\n",
    "            for epoch in epochs:\n",
    "                batches = self.create_train_batches(train_set)\n",
    "                main_index = 0\n",
    "                loss_sum = 0\n",
    "                # TODO: fix the printing\n",
    "                # Batches progress bar is deactivated until it is fixed. Relevant to \"nested bars issue\"\n",
    "                #_batches = tqdm(enumerate(batches), total=len(batches), desc=\"Batches\", position=1)\n",
    "                for index, batch_pair in enumerate(batches):\n",
    "                    loss_score = self.process_batch(batch_pair)\n",
    "                    main_index = main_index + batch_pair[0].num_graphs\n",
    "                    loss_sum = loss_sum + loss_score\n",
    "                    #_batches.update()\n",
    "\n",
    "                loss = loss_sum / main_index\n",
    "                loss_list.append(loss)\n",
    "\n",
    "                epochs.set_description(\"Epoch (Loss=%g)\" % round(loss, 5))\n",
    "                \n",
    "    def globalScore(self):\n",
    "        \"\"\"\n",
    "        Simple scoring\n",
    "        \"\"\"\n",
    "        print(\"\\n\\nModel evaluation.\\n\")\n",
    "        self.model = self.model.cpu()\n",
    "        self.model.eval()\n",
    "\n",
    "        # tests.debugModelStateDict(self.model)\n",
    "\n",
    "        # filled with exponentiated normalized ged values\n",
    "        self.norm_ground_truth = np.empty((len(self.testing_graphs), len(self.training_graphs)))\n",
    "        self.norm_prediction_mat = np.empty((len(self.testing_graphs), len(self.training_graphs)))\n",
    "\n",
    "        self.scores = np.empty((len(self.testing_graphs), len(self.training_graphs)))\n",
    "        self.rho_list = []\n",
    "        self.tau_list = []\n",
    "        self.prec_at_10_list = []\n",
    "        self.prec_at_20_list = []\n",
    "\n",
    "        t = tqdm(total=len(self.testing_graphs) * len(self.training_graphs))\n",
    "\n",
    "        for i, g in enumerate(self.testing_graphs):\n",
    "            # source batch is a batch with 1 test graph repeated multiple times\n",
    "            source_batch = Batch.from_data_list([g] * len(self.training_graphs))\n",
    "            # target batch is a batch with every training graph\n",
    "            target_batch = Batch.from_data_list(self.training_graphs)\n",
    "\n",
    "            # Get ground truth\n",
    "            data = self.transform((source_batch, target_batch))\n",
    "            data[\"g1\"] = data[\"g1\"].cpu()\n",
    "            data[\"g2\"] = data[\"g2\"].cpu()\n",
    "            target = data[\"target\"].cpu()\n",
    "            self.norm_ground_truth[i] = target\n",
    "\n",
    "            # Get prediction\n",
    "            prediction = self.model(data)\n",
    "            self.norm_prediction_mat[i] = prediction.detach().numpy()\n",
    "\n",
    "            # Update metrics\n",
    "            self.scores[i] = torch.nn.functional.mse_loss(prediction, target, reduction='none').detach().numpy()\n",
    "            self.rho_list.append(calculate_ranking_correlation(spearmanr, self.norm_prediction_mat[i], self.norm_ground_truth[i]))\n",
    "            self.tau_list.append(calculate_ranking_correlation(kendalltau, self.norm_prediction_mat[i], self.norm_ground_truth[i]))\n",
    "            self.prec_at_10_list.append(calculate_prec_at_k(10, self.norm_prediction_mat[i], self.norm_ground_truth[i]))\n",
    "            self.prec_at_20_list.append(calculate_prec_at_k(20, self.norm_prediction_mat[i], self.norm_ground_truth[i]))\n",
    "\n",
    "            t.update(len(self.training_graphs))\n",
    "\n",
    "        # Calculate metrics\n",
    "        self.model_error = np.mean(self.scores)\n",
    "        self.rho = np.mean(self.rho_list)\n",
    "        self.tau = np.mean(self.tau_list)\n",
    "        self.prec_at_10 = np.mean(self.prec_at_10_list)\n",
    "        self.prec_at_20 = np.mean(self.prec_at_20_list)\n",
    "\n",
    "        paperMetrics = [self.model_error, self.rho, self.tau, self.prec_at_10, self.prec_at_20]\n",
    "        \n",
    "        print(\"\\nmse(10^-3): \" + str(round(paperMetrics[0] * 1000, 5)) + \".\")\n",
    "        print(\"Spearman's rho: \" + str(round(paperMetrics[1], 5)) + \".\")\n",
    "        print(\"Kendall's tau: \" + str(round(paperMetrics[2], 5)) + \".\")\n",
    "        print(\"p@10: \" + str(round(paperMetrics[3], 5)) + \".\")\n",
    "        print(\"p@20: \" + str(round(paperMetrics[4], 5)) + \".\")\n",
    "\n",
    "        # self.presentGraphsWithGEDs(entireDataset=False, withPrediction=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def main(args : list):\n",
    "    \"\"\"\n",
    "    Parsing command line parameters, reading data.\n",
    "    Fitting and scoring a SimGNN model.\n",
    "    \"\"\"\n",
    "    args = parameter_parser(args)\n",
    "    tab_printer(args)\n",
    "    trainer = SimGNNTrainer(args)\n",
    "    \n",
    "    \n",
    "    use_pretrained = args.load_path != None  # To determine whether a pre-trained model will be used.\n",
    "    \n",
    "    if use_pretrained is True:\n",
    "        print(\"Pre-trained mode: load an already fit state instead of training.\")\n",
    "        checkpoint = torch.load(args.load_path)\n",
    "        trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        #TRAIN\n",
    "        start = time.time()\n",
    "        trainer.fit()\n",
    "        end = time.time()\n",
    "        print(f\"\\nTotal Training Time: {end-start}\")\n",
    "        if args.save_path != None:\n",
    "            torch.save({\n",
    "                'model_state_dict': trainer.model.state_dict(),\n",
    "                'optimizer_state_dict': trainer.optimizer.state_dict()\n",
    "            }, args.save_path)\n",
    "    \n",
    "    #TEST\n",
    "    trainer.globalScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (Loss=0.02128):   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing dataset.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch (Loss=0.00589): 100%|██████████| 100/100 [00:12<00:00,  8.06it/s]\n",
      "  1%|▏         | 1120/78400 [00:00<00:09, 8009.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Training Time: 12.40739393234253\n",
      "\n",
      "\n",
      "Model evaluation.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78400/78400 [00:10<00:00, 7639.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mse(10^-3): 5.79031.\n",
      "Spearman's rho: 0.65839.\n",
      "Kendall's tau: 0.49844.\n",
      "p@10: 0.79786.\n",
      "p@20: 0.79643.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main(args=[\n",
    "    \"--dataset=AIDS700nef\",\n",
    "    \"--save-path=AIDS700nef.pt\",\n",
    "    \"--epochs=100\",\n",
    "    \"--batch-size=128\",\n",
    "    \"--histogram\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/AIDS700nef/raw/AIDS700nef/ged.pickle\", 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "    for (x, y), g in obj.items():\n",
    "        if x == 103:\n",
    "            print((x, y), g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
