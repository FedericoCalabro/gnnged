{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kbest Matching with LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite, shortest_paths\n",
    "import torch\n",
    "import dgl\n",
    "\n",
    "class GedLowerBound(object):\n",
    "    def __init__(self, g1, g2, lb_setting=0):\n",
    "        self.g1 = g1\n",
    "        self.g2 = g2\n",
    "        self.lb_setting = lb_setting\n",
    "        self.n1 = g1.num_nodes()\n",
    "        self.n2 = g2.num_nodes()\n",
    "        assert self.n1 <= self.n2\n",
    "        if g1.ndata['f'].shape[1] == 1:\n",
    "            self.has_node_label = False\n",
    "        else:\n",
    "            self.has_node_label = True\n",
    "\n",
    "    @staticmethod\n",
    "    def mc(sg1, sg2):\n",
    "        # calculate the ged between two aligned graphs\n",
    "        A = (sg1.adj() - sg2.adj()).coalesce().values()\n",
    "        A_ged = (A ** 2).sum().item()\n",
    "        F = sg1.ndata['f'] - sg2.ndata['f']\n",
    "        F_ged = (F ** 2).sum().item()\n",
    "        return (A_ged + F_ged) / 2.0\n",
    "\n",
    "    def label_set(self, left_nodes, right_nodes):\n",
    "        # sp.second_matching may be None.\n",
    "        # In this case, calculating sp.ged2 makes right_nodes None.\n",
    "        if right_nodes is None:\n",
    "            return None\n",
    "\n",
    "        # left_nodes could be [] when a full mapping is given\n",
    "        partial_n = len(left_nodes)\n",
    "        if partial_n == 0 and len(right_nodes) == self.n1:\n",
    "            left_nodes = list(range(self.n1))\n",
    "            partial_n = self.n1\n",
    "        assert partial_n == len(right_nodes) and partial_n <= self.n1\n",
    "\n",
    "        sub_g1 = self.g1.subgraph(left_nodes)\n",
    "        sub_g2 = self.g2.subgraph(right_nodes)\n",
    "        lb = self.mc(sub_g1, sub_g2)\n",
    "        # print(lb)\n",
    "\n",
    "        # num of edges\n",
    "        m1 = self.g1.num_edges() - self.n1 - sub_g1.num_edges()  # + len(left_nodes)\n",
    "        m2 = self.g2.num_edges() - self.n2 - sub_g2.num_edges()  # + len(right_nodes)\n",
    "        lb += abs(m1 - m2) / 2.0\n",
    "        # print(lb)\n",
    "\n",
    "        # node label\n",
    "        if (not self.has_node_label) or (partial_n == self.n1):  # this is a full mapping\n",
    "            lb += (self.n2 - self.n1)\n",
    "        else:\n",
    "            f1 = dgl.remove_nodes(self.g1, left_nodes).ndata['f'].sum(dim=0)\n",
    "            f2 = dgl.remove_nodes(self.g2, right_nodes).ndata['f'].sum(dim=0)\n",
    "            intersect = torch.min(f1, f2)\n",
    "            lb += (max(f1.sum().item(), f2.sum().item()) - intersect.sum().item())\n",
    "\n",
    "        return lb\n",
    "\n",
    "\n",
    "class Subspace(object):\n",
    "    def __init__(self, G, matching, res, I=None, O=None):\n",
    "        \"\"\"\n",
    "        G is the original graph (a complete networkx bipartite DiGraph with edge attribute \"weight\"),\n",
    "        and self.G is a view (not copy) of G.\n",
    "        In other words, self.G of all subspaces are the same object.\n",
    "\n",
    "        We use I (edges used) and O (edges not used) to describe the solution subspace,\n",
    "        When calculating the second best matching, we make a copy of G and edit it according to I and O.\n",
    "        Therefore, self.G is also a constant.\n",
    "\n",
    "        For each solution subspace, the best matching and its weight (res) is given for initialization.\n",
    "        Then apply get_second_matching to calculate the 2nd best matching,\n",
    "        by finding a minimum alternating cycle on the best matching in O(n^3).\n",
    "\n",
    "        Only the best matching of the initial full space is calculated by KM algorithm.\n",
    "        The best matching of the following subspaces comes from its father space's best or second best matching.\n",
    "        In other words, subspace split merely depends on finding second best matching.\n",
    "        \"\"\"\n",
    "        self.G = G\n",
    "        self.best_matching = matching\n",
    "        self.best_res = res\n",
    "        self.I = set() if I is None else I  # the set of nodes whose matching can't change: use (u, v) -> add u into I\n",
    "        self.O = [] if O is None else O  # the list of edges we can not use: do not use (u, v) -> append (u, v) into O\n",
    "        self.get_second_matching()\n",
    "        self.lb = None  # the lower bound ged of this subspace (depends on I)\n",
    "        self.ged = None  # the ged of best matching\n",
    "        self.ged2 = None  # the ged of 2nd-best matching\n",
    "\n",
    "    def __repr__(self):\n",
    "        best_res = \"1st matching: {} {}\".format(self.best_matching, self.best_res)\n",
    "        second_res = \"2nd matching: {} {}\".format(self.second_matching, self.second_res)\n",
    "        IO = \"I: {}\\tO: {}\\tbranch edge: {}\".format(self.I, self.O, self.branch_edge)\n",
    "        return best_res + \"\\n\" + second_res + \"\\n\" + IO\n",
    "\n",
    "    def get_second_matching(self):\n",
    "        \"\"\"\n",
    "        Solve the second best matching based on the (1st) best one.\n",
    "        Apply floyd and the single source bellman ford algorithm to find the minimum alternating cycle.\n",
    "\n",
    "        Reverse the direction of edges in best matching and set their weights to the opposite.\n",
    "        Direction: top->bottom  --> bottom->top\n",
    "        Weight: negative --> positive\n",
    "\n",
    "        For each edge (matching[u], u) in the best matching,\n",
    "        the edge itself and the shortest path from u to matching[u] forms an alternating cycle.\n",
    "        Recall that the edges in the best matching have positive weights, and the ones not in have negative weights.\n",
    "        Therefore, the weight (sum) of an alternating cycle denotes\n",
    "        the decrease of weight after applying it on the best matching,\n",
    "        which is always non-negative.\n",
    "        It is clear that we could apply the minimum weight alternating cycle on the best matching\n",
    "        to get the 2nd best one.\n",
    "        \"\"\"\n",
    "        G = self.G.copy()\n",
    "        matching = self.best_matching.copy()\n",
    "        n1 = len(matching)\n",
    "        n = G.number_of_nodes()\n",
    "        n2 = n - n1\n",
    "\n",
    "        for (u, v) in self.O:\n",
    "            G[u][v][\"weight\"] = float(\"inf\")\n",
    "\n",
    "        matched = [False] * n2\n",
    "        for u in range(n1):\n",
    "            v = matching[u]\n",
    "            matched[v] = True\n",
    "            v += n1\n",
    "            w = -G[u][v][\"weight\"]  # become positive\n",
    "            if u in self.I:\n",
    "                w = float(\"inf\")\n",
    "            G.remove_edge(u, v)\n",
    "            G.add_edge(v, u, weight=w)\n",
    "\n",
    "        \"\"\"\n",
    "        Add a virtual node n.\n",
    "        For each bottom node v, add an edge between v and n whose weight is 0:\n",
    "        The direction is (n -> v) if v has been matched else (v -> n),\n",
    "        i.e., unmatched bottom nodes -> n -> matched bottom nodes.\n",
    "        \"\"\"\n",
    "        G.add_node(n, bipartite=0)\n",
    "        for v in range(n2):\n",
    "            if matched[v]:\n",
    "                G.add_edge(n, n1 + v, weight=0.0)\n",
    "            else:\n",
    "                G.add_edge(n1 + v, n, weight=0.0)\n",
    "\n",
    "        dis = shortest_paths.dense.floyd_warshall(G)\n",
    "        cycle_min_weight = float(\"inf\")\n",
    "        cycle_min_uv = None\n",
    "        for u in range(n1):\n",
    "            if u in self.I:\n",
    "                continue\n",
    "            v = matching[u] + n1\n",
    "            res = dis[u][v] + G[v][u][\"weight\"]\n",
    "            if res < cycle_min_weight:\n",
    "                cycle_min_weight = res\n",
    "                cycle_min_uv = (u, v)\n",
    "\n",
    "        if cycle_min_uv is None:\n",
    "            # the second best matching does not exist in this subspace\n",
    "            self.second_matching = None\n",
    "            self.second_res = None\n",
    "            self.branch_edge = None\n",
    "            return\n",
    "\n",
    "        u, v = cycle_min_uv\n",
    "        length, path = shortest_paths.weighted.single_source_bellman_ford(G, source=u, target=v)\n",
    "        assert abs(length + G[v][u][\"weight\"] - cycle_min_weight) < 1e-12\n",
    "\n",
    "        # print(\"best matching:\", matching)\n",
    "        # print(cycle_min_weight, path)\n",
    "\n",
    "        self.branch_edge = (u, v)  # an edge in the best matching but not in the second best one\n",
    "        for i in range(0, len(path), 2):\n",
    "            u, v = path[i], path[i + 1] - n1\n",
    "            if u != n:\n",
    "                matching[u] = v\n",
    "        self.second_matching = matching\n",
    "        self.second_res = self.best_res - cycle_min_weight\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\"\n",
    "        Suppose the branching edge is (u, v), which is in self.best_matching but not in self.second_matching.\n",
    "        Then current solution space sp is further split by using (u, v) or not.\n",
    "        sp1: use (u,v), add u into I, sp1's best solution is the same as sp's.\n",
    "        sp2: do not use (u, v), append (u, v) into O, sp2's best solution is sp's second best solution.\n",
    "\n",
    "        We conduct an in-place update which makes sp becomes sp1, and return sp2 as a new subspace object.\n",
    "        sp1's second_matching is calculated by calling self.get_second_matching(),\n",
    "        sp2's second_matching is automatically calculated while object initialization.\n",
    "        \"\"\"\n",
    "        u, v = self.branch_edge\n",
    "\n",
    "        I = self.I.copy()\n",
    "        self.I.add(u)\n",
    "        O = self.O.copy()\n",
    "        O.append((u, v))\n",
    "\n",
    "        G = self.G  # needn't copy, all subspaces use the same G\n",
    "        second_matching = self.second_matching\n",
    "        self.second_matching = None\n",
    "        second_res = self.second_res\n",
    "        self.second_res = None\n",
    "\n",
    "        self.get_second_matching()\n",
    "        sp_new = Subspace(G, second_matching, second_res, I, O)\n",
    "        return sp_new\n",
    "\n",
    "\n",
    "class KBestMSolver(object):\n",
    "    \"\"\"\n",
    "    Maintain a sequence of disjoint subspaces whose union is the full space.\n",
    "    The best matching of the i-th subspace is exactly the i-th best matching of the full space.\n",
    "    Specifically, self.subspaces[0].best_matching is the best matching,\n",
    "    self.subspaces[1].best_matching is the second best matching,\n",
    "    and self.subspaces[k-1].best_matching is the k-th best matching respectively.\n",
    "\n",
    "    self.k is the length of self.subspaces. In another word, self.k-best matching have been solved.\n",
    "    Apply self.expand_subspaces() to get the (self.k+1)-th best matching\n",
    "    and maintain the subspaces structure accordingly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, g1, g2, pre_ged=None):\n",
    "        \"\"\"\n",
    "        Initially, self.subspaces[0] is the full space.\n",
    "        \"\"\"\n",
    "        G, best_matching, res = self.from_tensor_to_nx(a)\n",
    "        sp = Subspace(G, best_matching, res)\n",
    "\n",
    "        self.lb = GedLowerBound(g1, g2)  # lower bound function\n",
    "        self.lb_value = sp.lb = self.lb.label_set([], [])\n",
    "        sp.ged = self.lb.label_set([], sp.best_matching)\n",
    "        self.min_ged = sp.ged  # current best(minimum) solution, i.e., an upper bound\n",
    "        sp.ged2 = self.lb.label_set([], sp.second_matching)\n",
    "        self.set_min_ged(sp.ged2)  # Note that sp.ged2 may be None.\n",
    "\n",
    "        self.subspaces = [sp]\n",
    "        self.k = 1  # the length of self.subspaces\n",
    "        self.expandable = True\n",
    "\n",
    "        self.pre_ged = pre_ged\n",
    "\n",
    "    def set_min_ged(self, ged):\n",
    "        if ged is None:\n",
    "            return\n",
    "        if ged < self.min_ged:\n",
    "            self.min_ged = ged\n",
    "\n",
    "    ''' actually not useful\n",
    "    def cal_min_lb(self):\n",
    "    lb = float('inf')\n",
    "    for sp in self.subspaces:\n",
    "        if sp.second_matching is None:\n",
    "            # This subspace only has one matching, sp.best_matching.\n",
    "            lb = min(lb, sp.ged)\n",
    "        else:\n",
    "            lb = min(lb, sp.lb)\n",
    "    return lb\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def from_tensor_to_nx(A):\n",
    "        \"\"\"\n",
    "        A is a pytorch tensor whose shape is [n1, n2],\n",
    "        denoting the weight matrix of a complete bipartite graph with n1+n2 nodes.\n",
    "        Suppose the weights in A are non-negative.\n",
    "\n",
    "        Construct a directed (top->bottom) networkx graph G based on A.\n",
    "        0 ~ n1-1 are top nodes, and n1 ~ n1 + n2 -1 are bottom nodes.\n",
    "        !!! The weights of G are set as the opposite of A.\n",
    "\n",
    "        The maximum weight full matching is also solved for further subspaces construction.\n",
    "        \"\"\"\n",
    "        n1, n2 = A.shape\n",
    "        assert n1 <= n2\n",
    "        top_nodes = range(n1)\n",
    "        bottom_nodes = range(n1, n1 + n2)\n",
    "\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(top_nodes, bipartite=0)\n",
    "        G.add_nodes_from(bottom_nodes, bipartite=1)\n",
    "        A = A.tolist()\n",
    "        for u in top_nodes:\n",
    "            for v in bottom_nodes:\n",
    "                G.add_edge(u, v, weight=-A[u][v - n1])\n",
    "        # weight is set as -A[u][v] to get the maximum weight full matching\n",
    "\n",
    "        matching = bipartite.matching.minimum_weight_full_matching(G, top_nodes)\n",
    "        matching = [matching[u] - n1 for u in top_nodes]\n",
    "        res = 0  # the weight sum of best matching\n",
    "        for u in top_nodes:\n",
    "            v = matching[u]\n",
    "            res += A[u][v]\n",
    "\n",
    "        '''\n",
    "        for u in top_nodes:\n",
    "            for v in bottom_nodes:\n",
    "                G[u][v]['weight'] *= -1\n",
    "        # restore weight to be positive\n",
    "        '''\n",
    "\n",
    "        return G, matching, res\n",
    "\n",
    "    def expand_subspaces(self):\n",
    "        \"\"\"\n",
    "        Find the subspace whose second matching is the largest, i.e., the (k+1)th best matching.\n",
    "        Then split this subspace\n",
    "        \"\"\"\n",
    "        max_res = -1\n",
    "        max_spid = None\n",
    "\n",
    "        for spid, sp in enumerate(self.subspaces):\n",
    "            if sp.lb < self.min_ged and sp.second_res is not None and sp.second_res > max_res:\n",
    "                #if (self.pre_ged is not None) and (sp.lb < self.pre_ged):\n",
    "                max_res = sp.second_res\n",
    "                max_spid = spid\n",
    "\n",
    "        if max_spid is None:\n",
    "            self.expandable = False\n",
    "            return\n",
    "\n",
    "        sp = self.subspaces[max_spid]\n",
    "        sp_new = sp.split()\n",
    "        self.subspaces.append(sp_new)\n",
    "        self.k += 1\n",
    "\n",
    "        sp_new.lb = sp.lb\n",
    "        sp_new.ged = sp.ged2\n",
    "        sp_new.ged2 = self.lb.label_set([], sp_new.second_matching)\n",
    "        self.set_min_ged(sp_new.ged2)\n",
    "\n",
    "        left_nodes = list(sp.I)\n",
    "        right_nodes = [sp.best_matching[u] for u in left_nodes]\n",
    "        sp.lb = self.lb.label_set(left_nodes, right_nodes)\n",
    "        # sp.ged does not change since sp.best_matching does not change\n",
    "        sp.ged2 = self.lb.label_set([], sp.second_matching)\n",
    "        self.set_min_ged(sp.ged2)\n",
    "\n",
    "    def get_matching(self, k):  # k starts form 1\n",
    "        while self.k < k and self.expandable:\n",
    "            self.expand_subspaces()\n",
    "\n",
    "        if self.k < k:\n",
    "            return None, None, None\n",
    "        else:\n",
    "            sp = self.subspaces[k-1]\n",
    "            return sp.best_matching, sp.best_res, sp.ged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class AttentionModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN Attention Module to make a pass on graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.setup_weights()\n",
    "        self.init_parameters()\n",
    "\n",
    "    def setup_weights(self):\n",
    "        \"\"\"\n",
    "        Defining weights.\n",
    "        \"\"\"\n",
    "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.args.filters_3, self.args.filters_3))\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializing weights.\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a graph level representation.\n",
    "        :param embedding: Result of the GCN.\n",
    "        :return representation: A graph level representation vector.\n",
    "        \"\"\"\n",
    "        global_context = torch.mean(torch.matmul(embedding, self.weight_matrix), dim=0)\n",
    "        transformed_global = torch.tanh(global_context)\n",
    "        sigmoid_scores = torch.sigmoid(torch.mm(embedding, transformed_global.view(-1, 1)))\n",
    "        representation = torch.mm(torch.t(embedding), sigmoid_scores)\n",
    "        return representation\n",
    "\n",
    "\n",
    "class TensorNetworkModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN Tensor Network module to calculate similarity vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, input_dim=None):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(TensorNetworkModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.input_dim = self.args.filters_3 if (input_dim is None) else input_dim\n",
    "        self.setup_weights()\n",
    "        self.init_parameters()\n",
    "\n",
    "    def setup_weights(self):\n",
    "        \"\"\"\n",
    "        Defining weights.\n",
    "        \"\"\"\n",
    "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.input_dim, self.input_dim, self.args.tensor_neurons))\n",
    "\n",
    "        self.weight_matrix_block = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons, 2*self.input_dim))\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons, 1))\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializing weights.\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix_block)\n",
    "        torch.nn.init.xavier_uniform_(self.bias)\n",
    "\n",
    "    def forward(self, embedding_1, embedding_2):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a similarity vector.\n",
    "        :param embedding_1: Result of the 1st embedding after attention.\n",
    "        :param embedding_2: Result of the 2nd embedding after attention.\n",
    "        :return scores: A similarity score vector.\n",
    "        \"\"\"\n",
    "        scoring = torch.mm(torch.t(embedding_1), self.weight_matrix.view(self.input_dim, -1))\n",
    "        scoring = scoring.view(self.input_dim, self.args.tensor_neurons)\n",
    "        scoring = torch.mm(torch.t(scoring), embedding_2)\n",
    "        combined_representation = torch.cat((embedding_1, embedding_2))\n",
    "        block_scoring = torch.mm(self.weight_matrix_block, combined_representation)\n",
    "        scores = torch.nn.functional.relu(scoring + block_scoring + self.bias)\n",
    "        return scores\n",
    "\n",
    "\n",
    "class Mlp(torch.nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :param number_of_labels: Number of node labels.\n",
    "        \"\"\"\n",
    "        super(Mlp, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        layers = []\n",
    "        '''\n",
    "        while dim > 1:\n",
    "            layers.append(torch.nn.Linear(dim, dim // 2))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            dim = dim // 2\n",
    "        layers[-1] = torch.nn.Sigmoid()\n",
    "        '''\n",
    "\n",
    "        layers.append(torch.nn.Linear(dim, dim * 2))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(dim * 2, dim))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(dim, 1))\n",
    "        #layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "        self.model = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# from noah\n",
    "class MatchingModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Graph-to-graph Module to gather cross-graph information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(MatchingModule, self).__init__()\n",
    "        self.args = args\n",
    "        self.setup_weights()\n",
    "        self.init_parameters()\n",
    "\n",
    "    def setup_weights(self):\n",
    "        \"\"\"\n",
    "        Defining weights.\n",
    "        \"\"\"\n",
    "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.args.filters_3, self.args.filters_3))\n",
    "\n",
    "    def init_parameters(self):\n",
    "        \"\"\"\n",
    "        Initializing weights.\n",
    "        \"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a graph level representation.\n",
    "        :param embedding: Result of the GCN/GIN.\n",
    "        :return representation: A graph level representation vector.\n",
    "        \"\"\"\n",
    "        global_context = torch.sum(torch.matmul(embedding, self.weight_matrix), dim=0)\n",
    "        transformed_global = torch.tanh(global_context)\n",
    "        return transformed_global\n",
    "\n",
    "\n",
    "#from TaGSim\n",
    "class GraphAggregationLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=10, out_features=10):\n",
    "        super(GraphAggregationLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h_prime = torch.mm(adj, input)\n",
    "        return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature=1):\n",
    "    y = logits + sample_gumbel(logits.shape)\n",
    "    return torch.nn.functional.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1, hard=True):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [*, n_class]\n",
    "    return: flatten --> [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "\n",
    "    if not hard:\n",
    "        return y\n",
    "\n",
    "    shape = y.shape\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros(shape).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard\n",
    "\n",
    "'''\n",
    "def sinkhorn(a, r=1.0, num_iter=10):\n",
    "    assert len(a.shape) == 2\n",
    "    n1, n2 = a.shape\n",
    "    b = a if n1 <= n2 else a.t()\n",
    "\n",
    "    for i in range(num_iter * 2):\n",
    "        b = torch.exp(b / r)\n",
    "        b = b / b.sum(dim=0)\n",
    "        b = b.t()\n",
    "\n",
    "    return b if n1 <= n2 else b.t()\n",
    "'''\n",
    "def sinkhorn(a, r=0.1, num_iter=20):\n",
    "    assert len(a.shape) == 2\n",
    "    n1, n2 = a.shape\n",
    "    b = a if n1 <= n2 else a.t()\n",
    "\n",
    "    for i in range(num_iter * 2):\n",
    "        b = torch.exp(b / r)\n",
    "        b = b / b.sum(dim=0)\n",
    "        b = b.t()\n",
    "\n",
    "    b = (b.round() - b).detach() + b\n",
    "\n",
    "    return b if n1 <= n2 else b.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "class GedMatrixModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    GED matrix module.\n",
    "    d is the size of input feature;\n",
    "    k is the size of hidden layer.\n",
    "\n",
    "    Input: n1 * d, n2 * d\n",
    "    step 1 matmul: (n1 * d) matmul (k * d * d) matmul (n2 * d).t() -> k * n1 * n2\n",
    "    step 2 mlp(k, 2k, k, 1): k * n1 * n2 -> (n1n2) * k -> (n1n2) * 2k -> (n1n2) * k -> (n1n2) * 1 -> n1 * n2\n",
    "    Output: n1 * n2\n",
    "    \"\"\"\n",
    "    def __init__(self, d, k):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(GedMatrixModule, self).__init__()\n",
    "\n",
    "        self.d = d\n",
    "        self.k = k\n",
    "        self.init_weight_matrix()\n",
    "        self.init_mlp()\n",
    "\n",
    "    def init_weight_matrix(self):\n",
    "        \"\"\"\n",
    "        Define and initilize a weight matrix of size (k, d, d).\n",
    "        \"\"\"\n",
    "        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.k, self.d, self.d))\n",
    "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
    "\n",
    "    def init_mlp(self):\n",
    "        \"\"\"\n",
    "        Define a mlp: k -> 2*k -> k -> 1\n",
    "        \"\"\"\n",
    "        k = self.k\n",
    "        layers = []\n",
    "\n",
    "        layers.append(torch.nn.Linear(k, k * 2))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(k * 2, k))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(k, 1))\n",
    "        # layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, embedding_1, embedding_2):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a similar matrix.\n",
    "        :param embedding_1: GCN(graph1) of size (n1, d)\n",
    "        :param embedding_2: GCN(graph2) of size (n2, d)\n",
    "        :return result: a similar matrix of size (n1, n2)\n",
    "        \"\"\"\n",
    "        n1, d1 = embedding_1.shape\n",
    "        n2, d2 = embedding_2.shape\n",
    "        assert d1 == self.d == d2\n",
    "\n",
    "        matrix = torch.matmul(embedding_1, self.weight_matrix)\n",
    "        matrix = torch.matmul(matrix, embedding_2.t())\n",
    "        matrix = matrix.reshape(self.k, -1).t()\n",
    "        matrix = self.mlp(matrix)\n",
    "\n",
    "        return matrix.reshape(n1, n2)\n",
    "\n",
    "\n",
    "class SimpleMatrixModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Simple matrix module.\n",
    "    d is the size of input feature;\n",
    "    k is the size of hidden layer.\n",
    "\n",
    "    Input: n1 * d, n2 * d\n",
    "    step 1 matmul: (n1 * d) matmul (k * d * d) matmul (n2 * d).t() -> k * n1 * n2\n",
    "    step 2 mlp(k, 2k, k, 1): k * n1 * n2 -> (n1n2) * k -> (n1n2) * 2k -> (n1n2) * k -> (n1n2) * 1 -> n1 * n2\n",
    "    Output: n1 * n2\n",
    "    \"\"\"\n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        super(SimpleMatrixModule, self).__init__()\n",
    "\n",
    "        self.k = k\n",
    "        self.init_mlp()\n",
    "\n",
    "    def init_mlp(self):\n",
    "        \"\"\"\n",
    "        Define a mlp: k -> 2*k -> k -> 1\n",
    "        \"\"\"\n",
    "        k = self.k\n",
    "        layers = []\n",
    "\n",
    "        layers.append(torch.nn.Linear(k, k * 2))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(k * 2, k))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(k, 1))\n",
    "        # layers.append(torch.nn.Sigmoid())\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, embedding_1, embedding_2):\n",
    "        \"\"\"\n",
    "        Making a forward propagation pass to create a similar matrix.\n",
    "        :param embedding_1: GCN(graph1) of size (n1, d)\n",
    "        :param embedding_2: GCN(graph2) of size (n2, d)\n",
    "        :return result: a similar matrix of size (n1, n2)\n",
    "        \"\"\"\n",
    "        n1, d1 = embedding_1.shape\n",
    "        n2, d2 = embedding_2.shape\n",
    "        assert d1 == self.k == d2\n",
    "\n",
    "        tmp_1 = embedding_1.unsqueeze(1).repeat(1, n2, 1)  # n1*d -> n1 1 d -> n1 n2 d\n",
    "        tmp_2 = embedding_2.unsqueeze(0).repeat(n1, 1, 1)  # n2*d -> 1 n2 d -> n1 n2 d\n",
    "        matrix = (tmp_1.reshape([n1 * n2, -1]) * tmp_2.reshape([n1 * n2, -1])).reshape([n1 * n2, -1])\n",
    "\n",
    "        matrix = self.mlp(matrix)\n",
    "\n",
    "        return matrix.reshape(n1, n2)\n",
    "\n",
    "def fixed_mapping_loss(mapping, gt_mapping):\n",
    "    mapping_loss = torch.nn.BCEWithLogitsLoss()\n",
    "    n1, n2 = mapping.shape\n",
    "\n",
    "    epoch_percent = 0.5\n",
    "    if epoch_percent >= 1.0:\n",
    "        return mapping_loss(mapping, gt_mapping)\n",
    "\n",
    "    num_1 = gt_mapping.sum().item()\n",
    "    num_0 = n1 * n2 - num_1\n",
    "    if num_1 >= num_0: # There is no need to use mask. Directly return the complete loss.\n",
    "        return mapping_loss(mapping, gt_mapping)\n",
    "\n",
    "    p_base = num_1 / num_0\n",
    "    p = 1.0 - (p_base + epoch_percent * (1-p_base))\n",
    "\n",
    "    #p = 1.0 - (epoch_num + 1.0) / 10\n",
    "    mask = (torch.rand([n1, n2], device=gt_mapping.device) + gt_mapping) > p\n",
    "    return mapping_loss(mapping[mask], gt_mapping[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import GCNConv\n",
    "\n",
    "class SimGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n",
    "    https://arxiv.org/abs/1808.05689\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, number_of_labels):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :param number_of_labels: Number of node labels.\n",
    "        \"\"\"\n",
    "        super(SimGNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.number_labels = number_of_labels\n",
    "        self.setup_layers()\n",
    "\n",
    "    def calculate_bottleneck_features(self):\n",
    "        \"\"\"\n",
    "        Deciding the shape of the bottleneck layer.\n",
    "        \"\"\"\n",
    "        if self.args.histogram:\n",
    "            self.feature_count = self.args.tensor_neurons + self.args.bins\n",
    "        else:\n",
    "            self.feature_count = self.args.tensor_neurons\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layers.\n",
    "        \"\"\"\n",
    "        self.calculate_bottleneck_features()\n",
    "        self.convolution_1 = GCNConv(self.number_labels, self.args.filters_1)\n",
    "        self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)\n",
    "        self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)\n",
    "\n",
    "        # bias\n",
    "        self.attention = AttentionModule(self.args)\n",
    "        self.tensor_network = TensorNetworkModule(self.args)\n",
    "\n",
    "        self.fully_connected_first = torch.nn.Linear(self.feature_count, self.args.bottle_neck_neurons)\n",
    "        self.fully_connected_second = torch.nn.Linear(self.args.bottle_neck_neurons, self.args.bottle_neck_neurons_2)\n",
    "        self.fully_connected_third = torch.nn.Linear(self.args.bottle_neck_neurons_2, self.args.bottle_neck_neurons_3)\n",
    "        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons_3, 1)\n",
    "        # self.bias_model = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def calculate_histogram(self, abstract_features_1, abstract_features_2):\n",
    "        \"\"\"\n",
    "        Calculate histogram from similarity matrix.\n",
    "        :param abstract_features_1: Feature matrix for graph 1.\n",
    "        :param abstract_features_2: Feature matrix for graph 2.\n",
    "        :return hist: Histsogram of similarity scores.\n",
    "        \"\"\"\n",
    "        scores = torch.mm(abstract_features_1, abstract_features_2).detach()\n",
    "        scores = scores.view(-1, 1)\n",
    "        hist = torch.histc(scores, bins=self.args.bins)\n",
    "        hist = hist / torch.sum(hist)\n",
    "        hist = hist.view(1, -1)\n",
    "        return hist\n",
    "\n",
    "    def convolutional_pass(self, edge_index, features):\n",
    "        \"\"\"\n",
    "        Making convolutional pass.\n",
    "        :param edge_index: Edge indices.\n",
    "        :param features: Feature matrix.\n",
    "        :return features: Abstract feature matrix.\n",
    "        \"\"\"\n",
    "        features = self.convolution_1(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features, p=self.args.dropout, training=self.training)\n",
    "\n",
    "        features = self.convolution_2(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features, p=self.args.dropout, training=self.training)\n",
    "\n",
    "        features = self.convolution_3(features, edge_index)\n",
    "        # features = torch.sigmoid(features)\n",
    "        return features\n",
    "\n",
    "    def ntn_pass(self, abstract_features_1, abstract_features_2):\n",
    "        pooled_features_1 = self.attention(abstract_features_1)\n",
    "        pooled_features_2 = self.attention(abstract_features_2)\n",
    "        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n",
    "        scores = torch.t(scores)\n",
    "        return scores\n",
    "\n",
    "    def forward(self, data, return_ged=False):\n",
    "        \"\"\"\n",
    "        Forward pass with graphs.\n",
    "        :param data: Data dictionary.\n",
    "        :param is_testing: pass\n",
    "        :param predict_value: pass\n",
    "        :return score: Similarity score.\n",
    "        \"\"\"\n",
    "        edge_index_1 = data[\"edge_index_1\"]\n",
    "        edge_index_2 = data[\"edge_index_2\"]\n",
    "        features_1 = data[\"features_1\"]\n",
    "        features_2 = data[\"features_2\"]\n",
    "\n",
    "        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1)\n",
    "        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2)\n",
    "\n",
    "        scores = self.ntn_pass(abstract_features_1, abstract_features_2)\n",
    "\n",
    "        if self.args.histogram == True:\n",
    "            hist = self.calculate_histogram(abstract_features_1, torch.t(abstract_features_2))\n",
    "            scores = torch.cat((scores, hist), dim=1).view(1, -1)\n",
    "\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_first(scores))\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_second(scores))\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_third(scores))\n",
    "        score = torch.sigmoid(self.scoring_layer(scores).view(-1))\n",
    "\n",
    "        if self.args.target_mode == \"exp\":\n",
    "            pre_ged = -torch.log(score) * data[\"avg_v\"]\n",
    "        elif self.args.target_mode == \"linear\":\n",
    "            pre_ged = score * data[\"hb\"]\n",
    "        else:\n",
    "            assert False\n",
    "        return score, pre_ged.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn.conv import GCNConv, GINConv\n",
    "\n",
    "class GPN(torch.nn.Module):\n",
    "    def __init__(self, args, number_of_labels):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :param number_of_labels: Number of node labels.\n",
    "        \"\"\"\n",
    "        super(GPN, self).__init__()\n",
    "        self.args = args\n",
    "        self.number_labels = number_of_labels\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layers.\n",
    "        \"\"\"\n",
    "        self.args.gnn_operator = 'gin'\n",
    "\n",
    "        if self.args.gnn_operator == 'gcn':\n",
    "            self.convolution_1 = GCNConv(self.number_labels, self.args.filters_1)\n",
    "            self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)\n",
    "            self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)\n",
    "        elif self.args.gnn_operator == 'gin':\n",
    "            nn1 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.number_labels, self.args.filters_1),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.args.filters_1, self.args.filters_1),\n",
    "                torch.nn.BatchNorm1d(self.args.filters_1))\n",
    "\n",
    "            nn2 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.args.filters_1, self.args.filters_2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.args.filters_2, self.args.filters_2),\n",
    "                torch.nn.BatchNorm1d(self.args.filters_2))\n",
    "\n",
    "            nn3 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.args.filters_2, self.args.filters_3),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.args.filters_3, self.args.filters_3),\n",
    "                torch.nn.BatchNorm1d(self.args.filters_3))\n",
    "\n",
    "            self.convolution_1 = GINConv(nn1, train_eps=True)\n",
    "            self.convolution_2 = GINConv(nn2, train_eps=True)\n",
    "            self.convolution_3 = GINConv(nn3, train_eps=True)\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown GNN-Operator.')\n",
    "\n",
    "        self.matching_1 = MatchingModule(self.args)\n",
    "        self.matching_2 = MatchingModule(self.args)\n",
    "        self.attention = AttentionModule(self.args)\n",
    "        self.tensor_network = TensorNetworkModule(self.args)\n",
    "        self.fully_connected_first = torch.nn.Linear(self.args.tensor_neurons, self.args.bottle_neck_neurons)\n",
    "        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons, 1)\n",
    "\n",
    "    def convolutional_pass(self, edge_index, features):\n",
    "        \"\"\"\n",
    "        Making convolutional pass.\n",
    "        :param edge_index: Edge indices.\n",
    "        :param features: Feature matrix.\n",
    "        :return features: Absstract feature matrix.\n",
    "        \"\"\"\n",
    "        features = self.convolution_1(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        # using_dropout = self.training\n",
    "        using_dropout = False\n",
    "        features = torch.nn.functional.dropout(features, p=self.args.dropout, training=using_dropout)\n",
    "        features = self.convolution_2(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features, p=self.args.dropout, training=using_dropout)\n",
    "        features = self.convolution_3(features, edge_index)\n",
    "        return features\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass with graphs.\n",
    "        :param data: Data dictionary.\n",
    "        :return score: Similarity score.\n",
    "        \"\"\"\n",
    "        edge_index_1 = data[\"edge_index_1\"]\n",
    "        edge_index_2 = data[\"edge_index_2\"]\n",
    "        features_1 = data[\"features_1\"]\n",
    "        features_2 = data[\"features_2\"]\n",
    "        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1)\n",
    "        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2)\n",
    "\n",
    "        tmp_feature_1 = abstract_features_1\n",
    "        tmp_feature_2 = abstract_features_2\n",
    "\n",
    "        abstract_features_1 = torch.sub(tmp_feature_1, self.matching_2(tmp_feature_2))\n",
    "        abstract_features_2 = torch.sub(tmp_feature_2, self.matching_1(tmp_feature_1))\n",
    "\n",
    "        abstract_features_1 = torch.abs(abstract_features_1)\n",
    "        abstract_features_2 = torch.abs(abstract_features_2)\n",
    "\n",
    "        pooled_features_1 = self.attention(abstract_features_1)\n",
    "        pooled_features_2 = self.attention(abstract_features_2)\n",
    "\n",
    "        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n",
    "        scores = torch.t(scores)\n",
    "\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_first(scores))\n",
    "        score = torch.sigmoid(self.scoring_layer(scores)).view(-1)\n",
    "        if self.args.target_mode == \"exp\":\n",
    "            pre_ged = -torch.log(score) * data[\"avg_v\"]\n",
    "        elif self.args.target_mode == \"linear\":\n",
    "            pre_ged = score * data[\"hb\"]\n",
    "        else:\n",
    "            assert False\n",
    "        return score, pre_ged.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GedGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn.conv import GCNConv, GINConv\n",
    "\n",
    "class GedGNN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n",
    "    https://arxiv.org/abs/1808.05689\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, number_of_labels):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        :param number_of_labels: Number of node labels.\n",
    "        \"\"\"\n",
    "        super(GedGNN, self).__init__()\n",
    "        self.args = args\n",
    "        self.number_labels = number_of_labels\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        \"\"\"\n",
    "        Creating the layers.\n",
    "        \"\"\"\n",
    "        self.args.gnn_operator = 'gin'\n",
    "\n",
    "        if self.args.gnn_operator == 'gcn':\n",
    "            self.convolution_1 = GCNConv(self.number_labels, self.args.filters_1)\n",
    "            self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)\n",
    "            self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)\n",
    "        elif self.args.gnn_operator == 'gin':\n",
    "            nn1 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.number_labels, self.args.filters_1),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.args.filters_1, self.args.filters_1),\n",
    "                torch.nn.BatchNorm1d(self.args.filters_1, track_running_stats=False))\n",
    "\n",
    "            nn2 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.args.filters_1, self.args.filters_2),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.args.filters_2, self.args.filters_2),\n",
    "                torch.nn.BatchNorm1d(self.args.filters_2, track_running_stats=False))\n",
    "\n",
    "            nn3 = torch.nn.Sequential(\n",
    "                torch.nn.Linear(self.args.filters_2, self.args.filters_3),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(self.args.filters_3, self.args.filters_3),\n",
    "                torch.nn.BatchNorm1d(self.args.filters_3, track_running_stats=False))\n",
    "\n",
    "            self.convolution_1 = GINConv(nn1, train_eps=True)\n",
    "            self.convolution_2 = GINConv(nn2, train_eps=True)\n",
    "            self.convolution_3 = GINConv(nn3, train_eps=True)\n",
    "        else:\n",
    "            raise NotImplementedError('Unknown GNN-Operator.')\n",
    "\n",
    "        self.mapMatrix = GedMatrixModule(self.args.filters_3, self.args.hidden_dim)\n",
    "        self.costMatrix = GedMatrixModule(self.args.filters_3, self.args.hidden_dim)\n",
    "        # self.costMatrix = SimpleMatrixModule(self.args.filters_3)\n",
    "\n",
    "        # bias\n",
    "        self.attention = AttentionModule(self.args)\n",
    "        self.tensor_network = TensorNetworkModule(self.args)\n",
    "\n",
    "        self.fully_connected_first = torch.nn.Linear(self.args.tensor_neurons, self.args.bottle_neck_neurons)\n",
    "        self.fully_connected_second = torch.nn.Linear(self.args.bottle_neck_neurons, self.args.bottle_neck_neurons_2)\n",
    "        self.fully_connected_third = torch.nn.Linear(self.args.bottle_neck_neurons_2, self.args.bottle_neck_neurons_3)\n",
    "        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons_3, 1)\n",
    "        # self.bias_model = torch.nn.Linear(2, 1)\n",
    "\n",
    "    def convolutional_pass(self, edge_index, features):\n",
    "        \"\"\"\n",
    "        Making convolutional pass.\n",
    "        :param edge_index: Edge indices.\n",
    "        :param features: Feature matrix.\n",
    "        :return features: Abstract feature matrix.\n",
    "        \"\"\"\n",
    "        features = self.convolution_1(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features, p=self.args.dropout, training=self.training)\n",
    "\n",
    "        features = self.convolution_2(features, edge_index)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        features = torch.nn.functional.dropout(features, p=self.args.dropout, training=self.training)\n",
    "\n",
    "        features = self.convolution_3(features, edge_index)\n",
    "        # features = torch.sigmoid(features)\n",
    "        return features\n",
    "\n",
    "    def get_bias_value(self, abstract_features_1, abstract_features_2):\n",
    "        pooled_features_1 = self.attention(abstract_features_1)\n",
    "        pooled_features_2 = self.attention(abstract_features_2)\n",
    "        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n",
    "        scores = torch.t(scores)\n",
    "\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_first(scores))\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_second(scores))\n",
    "        scores = torch.nn.functional.relu(self.fully_connected_third(scores))\n",
    "        score = self.scoring_layer(scores).view(-1)\n",
    "        return score\n",
    "\n",
    "    @staticmethod\n",
    "    def ged_from_mapping(matrix, A1, A2, f1, f2):\n",
    "        # edge loss\n",
    "        A_loss = torch.mm(torch.mm(matrix.t(), A1), matrix) - A2\n",
    "        # label loss\n",
    "        F_loss = torch.mm(matrix.t(), f1) - f2\n",
    "        mapping_ged = ((A_loss * A_loss).sum() + (F_loss * F_loss).sum()) / 2.0\n",
    "        return mapping_ged.view(-1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass with graphs.\n",
    "        :param data: Data dictionary.\n",
    "        :param is_testing: whether return ged value together with ged score\n",
    "        :return score: Similarity score.\n",
    "        \"\"\"\n",
    "        edge_index_1 = data[\"edge_index_1\"]\n",
    "        edge_index_2 = data[\"edge_index_2\"]\n",
    "        features_1 = data[\"features_1\"]\n",
    "        features_2 = data[\"features_2\"]\n",
    "\n",
    "        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1)\n",
    "        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2)\n",
    "\n",
    "        cost_matrix = self.costMatrix(abstract_features_1, abstract_features_2)\n",
    "        map_matrix = self.mapMatrix(abstract_features_1, abstract_features_2)\n",
    "\n",
    "        # calculate ged using map_matrix\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "        soft_matrix = m(map_matrix) * cost_matrix\n",
    "        bias_value = self.get_bias_value(abstract_features_1, abstract_features_2)\n",
    "        score = torch.sigmoid(soft_matrix.sum() + bias_value)\n",
    "\n",
    "        if self.args.target_mode == \"exp\":\n",
    "            pre_ged = -torch.log(score) * data[\"avg_v\"]\n",
    "        elif self.args.target_mode == \"linear\":\n",
    "            pre_ged = score * data[\"hb\"]\n",
    "        else:\n",
    "            assert False\n",
    "        return score, pre_ged.item(), map_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaGSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TaGSim(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    TaGSim: Type-aware Graph Similarity Learning and Computation\n",
    "    https://github.com/jiyangbai/TaGSim\n",
    "    \"\"\"\n",
    "    def __init__(self, args, number_of_labels):\n",
    "        super(TaGSim, self).__init__()\n",
    "        self.args = args\n",
    "        self.number_labels = number_of_labels\n",
    "        self.setup_layers()\n",
    "\n",
    "    def setup_layers(self):\n",
    "        self.gal1 = GraphAggregationLayer()\n",
    "        self.gal2 = GraphAggregationLayer()\n",
    "        self.feature_count = self.args.tensor_neurons\n",
    "\n",
    "        self.tensor_network_nc = TensorNetworkModule(self.args, 2 * self.number_labels)\n",
    "        self.tensor_network_in = TensorNetworkModule(self.args, 2 * self.number_labels)\n",
    "        self.tensor_network_ie = TensorNetworkModule(self.args, 2 * self.number_labels)\n",
    "\n",
    "        self.fully_connected_first_nc = torch.nn.Linear(self.feature_count, self.args.bottle_neck_neurons)\n",
    "        self.fully_connected_second_nc = torch.nn.Linear(self.args.bottle_neck_neurons, 8)\n",
    "        self.fully_connected_third_nc = torch.nn.Linear(8, 4)\n",
    "        self.scoring_layer_nc = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.fully_connected_first_in = torch.nn.Linear(self.feature_count, self.args.bottle_neck_neurons)\n",
    "        self.fully_connected_second_in = torch.nn.Linear(self.args.bottle_neck_neurons, 8)\n",
    "        self.fully_connected_third_in = torch.nn.Linear(8, 4)\n",
    "        self.scoring_layer_in = torch.nn.Linear(4, 1)\n",
    "\n",
    "        self.fully_connected_first_ie = torch.nn.Linear(self.feature_count, self.args.bottle_neck_neurons)\n",
    "        self.fully_connected_second_ie = torch.nn.Linear(self.args.bottle_neck_neurons, 8)\n",
    "        self.fully_connected_third_ie = torch.nn.Linear(8, 4)\n",
    "        self.scoring_layer_ie = torch.nn.Linear(4, 1)\n",
    "\n",
    "    def gal_pass(self, edge_index, features):\n",
    "        hidden1 = self.gal1(features, edge_index)\n",
    "        hidden2 = self.gal2(hidden1, edge_index)\n",
    "\n",
    "        return hidden1, hidden2\n",
    "\n",
    "    def forward(self, data):\n",
    "        edge_index_1 = data[\"edge_index_1\"]\n",
    "        edge_index_2 = data[\"edge_index_2\"]\n",
    "        features_1 = data[\"features_1\"]\n",
    "        features_2 = data[\"features_2\"]\n",
    "        n1, n2 = data[\"n1\"], data[\"n2\"]\n",
    "\n",
    "        adj_1 = torch.sparse_coo_tensor(edge_index_1, torch.ones(edge_index_1.shape[1]), (n1, n1)).to_dense()\n",
    "        adj_2 = torch.sparse_coo_tensor(edge_index_2, torch.ones(edge_index_2.shape[1]), (n2, n2)).to_dense()\n",
    "        # remove self-loops\n",
    "        adj_1 = adj_1 * (1.0 - torch.eye(n1))\n",
    "        adj_2 = adj_2 * (1.0 - torch.eye(n2))\n",
    "\n",
    "        graph1_hidden1, graph1_hidden2 = self.gal_pass(adj_1, features_1)\n",
    "        graph2_hidden1, graph2_hidden2 = self.gal_pass(adj_2, features_2)\n",
    "\n",
    "        graph1_01concat = torch.cat([features_1, graph1_hidden1], dim=1)\n",
    "        graph2_01concat = torch.cat([features_2, graph2_hidden1], dim=1)\n",
    "        graph1_12concat = torch.cat([graph1_hidden1, graph1_hidden2], dim=1)\n",
    "        graph2_12concat = torch.cat([graph2_hidden1, graph2_hidden2], dim=1)\n",
    "\n",
    "        graph1_01pooled = torch.sum(graph1_01concat, dim=0).unsqueeze(1)\n",
    "        graph1_12pooled = torch.sum(graph1_12concat, dim=0).unsqueeze(1)\n",
    "        graph2_01pooled = torch.sum(graph2_01concat, dim=0).unsqueeze(1)\n",
    "        graph2_12pooled = torch.sum(graph2_12concat, dim=0).unsqueeze(1)\n",
    "\n",
    "        scores_nc = self.tensor_network_nc(graph1_01pooled, graph2_01pooled)\n",
    "        scores_nc = torch.t(scores_nc)\n",
    "\n",
    "        scores_nc = torch.nn.functional.relu(self.fully_connected_first_nc(scores_nc))\n",
    "        scores_nc = torch.nn.functional.relu(self.fully_connected_second_nc(scores_nc))\n",
    "        scores_nc = torch.nn.functional.relu(self.fully_connected_third_nc(scores_nc))\n",
    "        score_nc = torch.sigmoid(self.scoring_layer_nc(scores_nc))\n",
    "\n",
    "        scores_in = self.tensor_network_in(graph1_01pooled, graph2_01pooled)\n",
    "        scores_in = torch.t(scores_in)\n",
    "\n",
    "        scores_in = torch.nn.functional.relu(self.fully_connected_first_in(scores_in))\n",
    "        scores_in = torch.nn.functional.relu(self.fully_connected_second_in(scores_in))\n",
    "        scores_in = torch.nn.functional.relu(self.fully_connected_third_in(scores_in))\n",
    "        score_in = torch.sigmoid(self.scoring_layer_in(scores_in))\n",
    "\n",
    "        scores_ie = self.tensor_network_ie(graph1_12pooled, graph2_12pooled)\n",
    "        scores_ie = torch.t(scores_ie)\n",
    "\n",
    "        scores_ie = torch.nn.functional.relu(self.fully_connected_first_ie(scores_ie))\n",
    "        scores_ie = torch.nn.functional.relu(self.fully_connected_second_ie(scores_ie))\n",
    "        scores_ie = torch.nn.functional.relu(self.fully_connected_third_ie(scores_ie))\n",
    "        score_ie = torch.sigmoid(self.scoring_layer_ie(scores_ie))\n",
    "\n",
    "        score = torch.cat([score_nc.view(-1), score_in.view(-1), score_ie.view(-1)])\n",
    "        if self.args.target_mode == \"exp\":\n",
    "            pre_ged = -torch.log(score) * data[\"avg_v\"]\n",
    "        elif self.args.target_mode == \"linear\":\n",
    "            pre_ged = score * data[\"hb\"]\n",
    "        else:\n",
    "            assert False\n",
    "        return score, pre_ged.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename, isfile\n",
    "from os import makedirs\n",
    "from glob import glob\n",
    "import networkx as nx\n",
    "import json\n",
    "from texttable import Texttable\n",
    "\n",
    "def tab_printer(args):\n",
    "    \"\"\"\n",
    "    Function to print the logs in a nice tabular format.\n",
    "    :param args: Parameters used for the model.\n",
    "    \"\"\"\n",
    "    args = vars(args)\n",
    "    keys = sorted(args.keys())\n",
    "    t = Texttable()\n",
    "    rows = [[\"Parameter\", \"Value\"]] + [[k.replace(\"_\", \" \").capitalize(), args[k]] for k in keys]\n",
    "    t.add_rows(rows)\n",
    "    print(t.draw())\n",
    "\n",
    "def sorted_nicely(l):\n",
    "    \"\"\"\n",
    "    Sort file names in a fancy way.\n",
    "    The numbers in file names are extracted and converted from str into int first,\n",
    "    so file names can be sorted based on int comparison.\n",
    "    :param l: A list of file names:str.\n",
    "    :return: A nicely sorted file name list.\n",
    "    \"\"\"\n",
    "\n",
    "    def tryint(s):\n",
    "        try:\n",
    "            return int(s)\n",
    "        except:\n",
    "            return s\n",
    "\n",
    "    import re\n",
    "    def alphanum_key(s):\n",
    "        return [tryint(c) for c in re.split('([0-9]+)', s)]\n",
    "\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "def get_file_paths(dir, file_format='json'):\n",
    "    \"\"\"\n",
    "    Return all file paths with file_format under dir.\n",
    "    :param dir: Input path.\n",
    "    :param file_format: The suffix name of required files.\n",
    "    :return paths: The paths of all required files.\n",
    "    \"\"\"\n",
    "    dir = dir.rstrip('/')\n",
    "    paths = sorted_nicely(glob(dir + '/*.' + file_format))\n",
    "    return paths\n",
    "\n",
    "def iterate_get_graphs(dir, file_format):\n",
    "    \"\"\"\n",
    "    Read networkx (dict) graphs from all .gexf (.json) files under dir.\n",
    "    :param dir: Input path.\n",
    "    :param file_format: The suffix name of required files.\n",
    "    :return graphs: Networkx (dict) graphs.\n",
    "    \"\"\"\n",
    "    assert file_format in ['gexf', 'json', 'onehot', 'anchor']\n",
    "    graphs = []\n",
    "    for file in get_file_paths(dir, file_format):\n",
    "        gid = int(basename(file).split('.')[0])\n",
    "        if file_format == 'gexf':\n",
    "            g = nx.read_gexf(file)\n",
    "            g.graph['gid'] = gid\n",
    "            if not nx.is_connected(g):\n",
    "                raise RuntimeError('{} not connected'.format(gid))\n",
    "        elif file_format == 'json':\n",
    "            # g is a dict\n",
    "            g = json.load(open(file, 'r'))\n",
    "            g['gid'] = gid\n",
    "        elif file_format in ['onehot', 'anchor']:\n",
    "            # g is a list of onehot labels\n",
    "            g = json.load(open(file, 'r'))\n",
    "        graphs.append(g)\n",
    "    return graphs\n",
    "\n",
    "def load_all_graphs(data_location, dataset_name):\n",
    "    graphs = iterate_get_graphs(data_location + \"json_data/\" + dataset_name + \"/train\", \"json\")\n",
    "    train_num = len(graphs)\n",
    "    graphs += iterate_get_graphs(data_location + \"json_data/\" + dataset_name + \"/test\", \"json\")\n",
    "    test_num = len(graphs) - train_num\n",
    "    val_num = test_num\n",
    "    train_num -= val_num\n",
    "    return train_num, val_num, test_num, graphs\n",
    "\n",
    "def load_labels(data_location, dataset_name):\n",
    "    path = data_location + \"json_data/\" + dataset_name + \"/labels.json\"\n",
    "    global_labels = json.load(open(path, 'r'))\n",
    "    features = iterate_get_graphs(data_location + \"json_data/\" + dataset_name + \"/train\", \"onehot\") + iterate_get_graphs(data_location + \"json_data/\" + dataset_name + \"/test\", \"onehot\")\n",
    "    print('Load one-hot label features (dim = {}) of {}.'.format(len(global_labels), dataset_name))\n",
    "    return global_labels, features\n",
    "\n",
    "def load_ged(ged_dict, data_location='', dataset_name='AIDS', file_name='TaGED.json'):\n",
    "    '''\n",
    "    list(tuple)\n",
    "    ged = [(id_1, id_2, ged_value, ged_nc, ged_in, ged_ie, [best_node_mapping])]\n",
    "\n",
    "    id_1 and id_2 are the IDs of a graph pair, e.g., the ID of 4.json is 4.\n",
    "    The given graph pairs satisfy that n1 <= n2.\n",
    "\n",
    "    ged_value = ged_nc + ged_in + ged_ie\n",
    "    (ged_nc, ged_in, ged_ie) is the type-aware ged following the setting of TaGSim.\n",
    "    ged_nc: the number of node relabeling\n",
    "    ged_in: the number of node insertions/deletions\n",
    "    ged_ie: the number of edge insertions/deletions\n",
    "\n",
    "    [best_node_mapping] contains 10 best matching at most.\n",
    "    best_node_mapping is a list of length n1: u in g1 -> best_node_mapping[u] in g2\n",
    "\n",
    "    return dict()\n",
    "    ged_dict[(id_1, id_2)] = ((ged_value, ged_nc, ged_in, ged_ie), best_node_mapping_list)\n",
    "    '''\n",
    "    path = \"{}json_data/{}/{}\".format(data_location, dataset_name, file_name)\n",
    "    TaGED = json.load(open(path, 'r'))\n",
    "    for (id_1, id_2, ged_value, ged_nc, ged_in, ged_ie, mappings) in TaGED:\n",
    "        ta_ged = (ged_value, ged_nc, ged_in, ged_ie)\n",
    "        ged_dict[(id_1, id_2)] = (ta_ged, mappings)\n",
    "\n",
    "def load_features(data_location, dataset_name, feature_name):\n",
    "    features = iterate_get_graphs(data_location + \"json_data/\" + dataset_name + \"/train\", feature_name) + iterate_get_graphs(data_location + \"json_data/\" + dataset_name + \"/test\", feature_name)\n",
    "    feature_dim = len(features[0][0])\n",
    "    print('Load {} features (dim = {}) of {}.'.format(feature_name, feature_dim, dataset_name))\n",
    "    return feature_dim, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    A general model trainer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        \"\"\"\n",
    "        :param args: Arguments object.\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "        self.load_data_time = 0.0\n",
    "        self.to_torch_time = 0.0\n",
    "        self.results = []\n",
    "\n",
    "        self.use_gpu = torch.cuda.is_available()\n",
    "        print(\"use_gpu =\", self.use_gpu)\n",
    "        self.device = torch.device('cuda') if self.use_gpu else torch.device('cpu')\n",
    "\n",
    "        self.load_data()\n",
    "        self.transfer_data_to_torch()\n",
    "        self.delta_graphs = [None] * len(self.graphs)\n",
    "        # self.gen_delta_graphs()\n",
    "        self.init_graph_pairs()\n",
    "\n",
    "        self.setup_model()\n",
    "\n",
    "    def setup_model(self):\n",
    "        if self.args.model_name == 'GPN':\n",
    "            self.model = GPN(self.args, self.number_of_labels).to(self.device)\n",
    "        elif self.args.model_name == \"SimGNN\":\n",
    "            self.args.filters_1 = 64\n",
    "            self.args.filters_2 = 32\n",
    "            self.args.filters_3 = 16\n",
    "            self.args.histogram = True\n",
    "            self.args.target_mode = 'exp'\n",
    "            self.model = SimGNN(self.args, self.number_of_labels).to(self.device)\n",
    "        elif self.args.model_name == \"GedGNN\":\n",
    "            if self.args.dataset in [\"AIDS\", \"Linux\"]:\n",
    "                self.args.loss_weight = 10.0\n",
    "            else:\n",
    "                self.args.loss_weight = 1.0\n",
    "            # self.args.target_mode = 'exp'\n",
    "            self.args.gtmap = True\n",
    "            self.model = GedGNN(self.args, self.number_of_labels).to(self.device)\n",
    "        elif self.args.model_name == \"TaGSim\":\n",
    "            self.args.target_mode = 'exp'\n",
    "            self.model = TaGSim(self.args, self.number_of_labels).to(self.device)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass with a batch of data.\n",
    "        :param batch: Batch of graph pair locations.\n",
    "        :return loss: Loss on the batch.\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        losses = torch.tensor([0]).float().to(self.device)\n",
    "\n",
    "        if self.args.model_name in [\"GPN\", \"SimGNN\"]:\n",
    "            for graph_pair in batch:\n",
    "                data = self.pack_graph_pair(graph_pair)\n",
    "                target = data[\"target\"]\n",
    "                prediction, _ = self.model(data)\n",
    "                losses = losses + torch.nn.functional.mse_loss(target, prediction)\n",
    "                # self.values.append((target - prediction).item())\n",
    "        elif self.args.model_name == \"GedGNN\":\n",
    "            weight = self.args.loss_weight\n",
    "            for graph_pair in batch:\n",
    "                data = self.pack_graph_pair(graph_pair)\n",
    "                target, gt_mapping = data[\"target\"], data[\"mapping\"]\n",
    "                prediction, _, mapping = self.model(data)\n",
    "                losses = losses + fixed_mapping_loss(mapping, gt_mapping) + weight * F.mse_loss(target, prediction)\n",
    "                if self.args.finetune:\n",
    "                    if self.args.target_mode == \"linear\":\n",
    "                        losses = losses + F.relu(target - prediction)\n",
    "                    else: # \"exp\"\n",
    "                        losses = losses + F.relu(prediction - target)\n",
    "        elif self.args.model_name == \"TaGSim\":\n",
    "            for graph_pair in batch:\n",
    "                data = self.pack_graph_pair(graph_pair)\n",
    "                ta_ged = data[\"ta_ged\"]\n",
    "                prediction, _ = self.model(data)\n",
    "                losses = losses + torch.nn.functional.mse_loss(ta_ged, prediction)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        losses.backward()\n",
    "        self.optimizer.step()\n",
    "        return losses.item()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load graphs, ged and labels if needed.\n",
    "        self.ged: dict-dict, ged['graph_id_1']['graph_id_2'] stores the ged value.\n",
    "        \"\"\"\n",
    "        t1 = time.time()\n",
    "        dataset_name = self.args.dataset\n",
    "        self.train_num, self.val_num, self.test_num, self.graphs = load_all_graphs(self.args.abs_path, dataset_name)\n",
    "        print(\"Load {} graphs. ({} for training)\".format(len(self.graphs), self.train_num))\n",
    "\n",
    "        self.number_of_labels = 0\n",
    "        if dataset_name in ['AIDS']:\n",
    "            self.global_labels, self.features = load_labels(self.args.abs_path, dataset_name)\n",
    "            self.number_of_labels = len(self.global_labels)\n",
    "        if self.number_of_labels == 0:\n",
    "            self.number_of_labels = 1\n",
    "            self.features = []\n",
    "            for g in self.graphs:\n",
    "                self.features.append([[2.0] for u in range(g['n'])])\n",
    "        # print(self.global_labels)\n",
    "\n",
    "        ged_dict = dict()\n",
    "        # We could load ged info from several files.\n",
    "        # load_ged(ged_dict, self.args.abs_path, dataset_name, 'xxx.json')\n",
    "        load_ged(ged_dict, self.args.abs_path, dataset_name, 'TaGED.json')\n",
    "        self.ged_dict = ged_dict\n",
    "        print(\"Load ged dict.\")\n",
    "        # print(self.ged['2050']['30'])\n",
    "        t2 = time.time()\n",
    "        self.load_data_time = t2 - t1\n",
    "\n",
    "    def transfer_data_to_torch(self):\n",
    "        \"\"\"\n",
    "        Transfer loaded data to torch.\n",
    "        \"\"\"\n",
    "        t1 = time.time()\n",
    "\n",
    "        self.edge_index = []\n",
    "        # self.A = []\n",
    "        for g in self.graphs:\n",
    "            edge = g['graph']\n",
    "            edge = edge + [[y, x] for x, y in edge]\n",
    "            edge = edge + [[x, x] for x in range(g['n'])]\n",
    "            edge = torch.tensor(edge).t().long().to(self.device)\n",
    "            self.edge_index.append(edge)\n",
    "            # A = torch.sparse_coo_tensor(edge, torch.ones(edge.shape[1]), (g['n'], g['n'])).to_dense().to(self.device)\n",
    "            # self.A.append(A)\n",
    "\n",
    "        self.features = [torch.tensor(x).float().to(self.device) for x in self.features]\n",
    "        print(\"Feature shape of 1st graph:\", self.features[0].shape)\n",
    "\n",
    "        n = len(self.graphs)\n",
    "        mapping = [[None for i in range(n)] for j in range(n)]\n",
    "        ged = [[(0., 0., 0., 0.) for i in range(n)] for j in range(n)]\n",
    "        gid = [g['gid'] for g in self.graphs]\n",
    "        self.gid = gid\n",
    "        self.gn = [g['n'] for g in self.graphs]\n",
    "        self.gm = [g['m'] for g in self.graphs]\n",
    "        for i in tqdm(range(n), total=n, desc=f\"transfer_data_to_torch\"):\n",
    "        # for i in range(n):\n",
    "            # mapping[i][i] = torch.eye(self.gn[i], dtype=torch.float, device=self.device)\n",
    "            for j in range(i + 1, n):\n",
    "                id_pair = (gid[i], gid[j])\n",
    "                n1, n2 = self.gn[i], self.gn[j]\n",
    "                if id_pair not in self.ged_dict:\n",
    "                    id_pair = (gid[j], gid[i])\n",
    "                    n1, n2 = n2, n1\n",
    "                if id_pair not in self.ged_dict:\n",
    "                    ged[i][j] = ged[j][i] = None\n",
    "                    mapping[i][j] = mapping[j][i] = None\n",
    "                else:\n",
    "                    ta_ged, gt_mappings = self.ged_dict[id_pair]\n",
    "                    ged[i][j] = ged[j][i] = ta_ged\n",
    "                    # mapping_list = [[0 for y in range(n2)] for x in range(n1)]\n",
    "                    # for gt_mapping in gt_mappings:\n",
    "                    #     for x, y in enumerate(gt_mapping):\n",
    "                    #         mapping_list[x][y] = 1\n",
    "                    # mapping_matrix = torch.tensor(mapping_list).float().to(self.device)\n",
    "                    # mapping[i][j] = mapping[j][i] = mapping_matrix\n",
    "        self.ged = ged\n",
    "        self.mapping = mapping\n",
    "\n",
    "        t2 = time.time()\n",
    "        self.to_torch_time = t2 - t1\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_graph(g, f, device):\n",
    "        new_data = dict()\n",
    "\n",
    "        n = g['n']\n",
    "        permute = list(range(n))\n",
    "        random.shuffle(permute)\n",
    "        mapping = torch.sparse_coo_tensor((list(range(n)), permute), [1.0] * n, (n, n)).to_dense().to(device)\n",
    "\n",
    "        edge = g['graph']\n",
    "        edge_set = set()\n",
    "        for x, y in edge:\n",
    "            edge_set.add((x, y))\n",
    "            edge_set.add((y, x))\n",
    "\n",
    "        random.shuffle(edge)\n",
    "        m = len(edge)\n",
    "        ged = random.randint(1, 5) if n <= 20 else random.randint(1, 10)\n",
    "        del_num = min(m, random.randint(0, ged))\n",
    "        edge = edge[:(m - del_num)]  # the last del_num edges in edge are removed\n",
    "        add_num = ged - del_num\n",
    "        if (add_num + m) * 2 > n * (n - 1):\n",
    "            add_num = n * (n - 1) // 2 - m\n",
    "        cnt = 0\n",
    "        while cnt < add_num:\n",
    "            x = random.randint(0, n - 1)\n",
    "            y = random.randint(0, n - 1)\n",
    "            if (x != y) and (x, y) not in edge_set:\n",
    "                edge_set.add((x, y))\n",
    "                edge_set.add((y, x))\n",
    "                cnt += 1\n",
    "                edge.append([x, y])\n",
    "        assert len(edge) == m - del_num + add_num\n",
    "        new_data[\"n\"] = n\n",
    "        new_data[\"m\"] = len(edge)\n",
    "\n",
    "        new_edge = [[permute[x], permute[y]] for x, y in edge]\n",
    "        new_edge = new_edge + [[y, x] for x, y in new_edge]  # add reverse edges\n",
    "        new_edge = new_edge + [[x, x] for x in range(n)]  # add self-loops\n",
    "\n",
    "        new_edge = torch.tensor(new_edge).t().long().to(device)\n",
    "\n",
    "        feature2 = torch.zeros(f.shape).to(device)\n",
    "        for x, y in enumerate(permute):\n",
    "            feature2[y] = f[x]\n",
    "\n",
    "        new_data[\"permute\"] = permute\n",
    "        new_data[\"mapping\"] = mapping\n",
    "        ged = del_num + add_num\n",
    "        new_data[\"ta_ged\"] = (ged, 0, 0, ged)\n",
    "        new_data[\"edge_index\"] = new_edge\n",
    "        new_data[\"features\"] = feature2\n",
    "        return new_data\n",
    "\n",
    "    def gen_delta_graphs(self):\n",
    "        k = self.args.num_delta_graphs\n",
    "        n = len(self.graphs)\n",
    "        for i, g in enumerate(self.graphs):\n",
    "            # Do not generate delta graphs for small graphs.\n",
    "            if g['n'] <= 10:\n",
    "                continue\n",
    "            # gen k delta graphs\n",
    "            f = self.features[i]\n",
    "            self.delta_graphs[i] = [Trainer.delta_graph(g, f, self.device) for j in range(k)]\n",
    "\n",
    "    def check_pair(self, i, j):\n",
    "        if i == j:\n",
    "            return (0, i, j)\n",
    "        id1, id2 = self.gid[i], self.gid[j]\n",
    "        if (id1, id2) in self.ged_dict:\n",
    "            return (0, i, j)\n",
    "        elif (id2, id1) in self.ged_dict:\n",
    "            return (0, j, i)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def init_graph_pairs(self):\n",
    "        random.seed(1)\n",
    "\n",
    "        self.training_graphs = []\n",
    "        self.val_graphs = []\n",
    "        self.testing_graphs = []\n",
    "        self.testing2_graphs = []\n",
    "\n",
    "        train_num = self.train_num\n",
    "        val_num = train_num + self.val_num\n",
    "        test_num = len(self.graphs)\n",
    "\n",
    "        if self.args.demo:\n",
    "            train_num = 30\n",
    "            val_num = 40\n",
    "            test_num = 50\n",
    "            self.args.epochs = 1\n",
    "\n",
    "        assert self.args.graph_pair_mode == \"combine\"\n",
    "        dg = self.delta_graphs\n",
    "\n",
    "        TEMP_MAX = 1000\n",
    "\n",
    "        # for i in tqdm(range(train_num), total=train_num, desc=f\"initializing training graphs\"):\n",
    "        for i in range(train_num):\n",
    "            if self.gn[i] <= TEMP_MAX:\n",
    "                for j in range(i, train_num):\n",
    "                    tmp = self.check_pair(i, j)\n",
    "                    if tmp is not None:\n",
    "                        self.training_graphs.append(tmp)\n",
    "            elif dg[i] is not None:\n",
    "                k = len(dg[i])\n",
    "                for j in range(k):\n",
    "                    self.training_graphs.append((1, i, j))\n",
    "\n",
    "        li = []\n",
    "        for i in range(train_num):\n",
    "            if self.gn[i] <= TEMP_MAX:\n",
    "                li.append(i)\n",
    "\n",
    "        # for i in tqdm(range(train_num, val_num), total=train_num, desc=f\"initializing val graphs\"):\n",
    "        for i in range(train_num, val_num):\n",
    "            if self.gn[i] <= TEMP_MAX:\n",
    "                random.shuffle(li)\n",
    "                self.val_graphs.append((0, i, li[:self.args.num_testing_graphs]))\n",
    "            elif dg[i] is not None:\n",
    "                k = len(dg[i])\n",
    "                self.val_graphs.append((1, i, list(range(k))))\n",
    "\n",
    "        # for i in tqdm(range(val_num, test_num), total=train_num, desc=f\"initializing test graphs\"):\n",
    "        for i in range(val_num, test_num):\n",
    "            if self.gn[i] <= TEMP_MAX:\n",
    "                random.shuffle(li)\n",
    "                self.testing_graphs.append((0, i, li[:self.args.num_testing_graphs]))\n",
    "            elif dg[i] is not None:\n",
    "                k = len(dg[i])\n",
    "                self.testing_graphs.append((1, i, list(range(k))))\n",
    "\n",
    "        li = []\n",
    "        for i in range(val_num, test_num):\n",
    "            if self.gn[i] <= TEMP_MAX:\n",
    "                li.append(i)\n",
    "\n",
    "        # for i in tqdm(range(val_num, test_num), total=train_num, desc=f\"initializing test2 graphs\"):\n",
    "        for i in range(val_num, test_num):\n",
    "            if self.gn[i] <= TEMP_MAX:\n",
    "                random.shuffle(li)\n",
    "                self.testing2_graphs.append((0, i, li[:self.args.num_testing_graphs]))\n",
    "            elif dg[i] is not None:\n",
    "                k = len(dg[i])\n",
    "                self.testing2_graphs.append((1, i, list(range(k))))\n",
    "\n",
    "        print(\"Generate {} training graph pairs.\".format(len(self.training_graphs)))\n",
    "        print(\"Generate {} * {} val graph pairs.\".format(len(self.val_graphs), self.args.num_testing_graphs))\n",
    "        print(\"Generate {} * {} testing graph pairs.\".format(len(self.testing_graphs), self.args.num_testing_graphs))\n",
    "        print(\"Generate {} * {} testing2 graph pairs.\".format(len(self.testing2_graphs), self.args.num_testing_graphs))\n",
    "\n",
    "    def create_batches(self):\n",
    "        \"\"\"\n",
    "        Creating batches from the training graph list.\n",
    "        :return batches: List of lists with batches.\n",
    "        \"\"\"\n",
    "        random.shuffle(self.training_graphs)\n",
    "        batches = []\n",
    "        for graph in range(0, len(self.training_graphs), self.args.batch_size):\n",
    "            batches.append(self.training_graphs[graph:graph + self.args.batch_size])\n",
    "        return batches\n",
    "\n",
    "    def pack_graph_pair(self, graph_pair):\n",
    "        \"\"\"\n",
    "        Prepare the graph pair data for GedGNN model.\n",
    "        :param graph_pair: (pair_type, id_1, id_2)\n",
    "        :return new_data: Dictionary of Torch Tensors.\n",
    "        \"\"\"\n",
    "        new_data = dict()\n",
    "\n",
    "        (pair_type, id_1, id_2) = graph_pair\n",
    "        if pair_type == 0:  # normal case\n",
    "            gid_pair = (self.gid[id_1], self.gid[id_2])\n",
    "            if gid_pair not in self.ged_dict:\n",
    "                id_1, id_2 = (id_2, id_1)\n",
    "\n",
    "            real_ged = self.ged[id_1][id_2][0]\n",
    "            ta_ged = self.ged[id_1][id_2][1:]\n",
    "\n",
    "            new_data[\"id_1\"] = id_1\n",
    "            new_data[\"id_2\"] = id_2\n",
    "\n",
    "            new_data[\"edge_index_1\"] = self.edge_index[id_1]\n",
    "            new_data[\"edge_index_2\"] = self.edge_index[id_2]\n",
    "            new_data[\"features_1\"] = self.features[id_1]\n",
    "            new_data[\"features_2\"] = self.features[id_2]\n",
    "\n",
    "            if self.args.gtmap:\n",
    "                new_data[\"mapping\"] = self.mapping[id_1][id_2]\n",
    "        elif pair_type == 1:  # delta graphs\n",
    "            new_data[\"id\"] = id_1\n",
    "            dg: dict = self.delta_graphs[id_1][id_2]\n",
    "\n",
    "            real_ged = dg[\"ta_ged\"][0]\n",
    "            ta_ged = dg[\"ta_ged\"][1:]\n",
    "\n",
    "            new_data[\"edge_index_1\"] = self.edge_index[id_1]\n",
    "            new_data[\"edge_index_2\"] = dg[\"edge_index\"]\n",
    "            new_data[\"features_1\"] = self.features[id_1]\n",
    "            new_data[\"features_2\"] = dg[\"features\"]\n",
    "\n",
    "            if self.args.gtmap:\n",
    "                new_data[\"mapping\"] = dg[\"mapping\"]\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        n1, m1 = (self.gn[id_1], self.gm[id_1])\n",
    "        n2, m2 = (self.gn[id_2], self.gm[id_2]) if pair_type == 0 else (dg[\"n\"], dg[\"m\"])\n",
    "        new_data[\"n1\"] = n1\n",
    "        new_data[\"n2\"] = n2\n",
    "        new_data[\"ged\"] = real_ged\n",
    "        # new_data[\"ta_ged\"] = ta_ged\n",
    "        if self.args.target_mode == \"exp\":\n",
    "            avg_v = (n1 + n2) / 2.0\n",
    "            new_data[\"avg_v\"] = avg_v\n",
    "            new_data[\"target\"] = torch.exp(torch.tensor([-real_ged / avg_v]).float()).to(self.device)\n",
    "            new_data[\"ta_ged\"] = torch.exp(torch.tensor(ta_ged).float() / -avg_v).to(self.device)\n",
    "        elif self.args.target_mode == \"linear\":\n",
    "            higher_bound = max(n1, n2) + max(m1, m2)\n",
    "            new_data[\"hb\"] = higher_bound\n",
    "            new_data[\"target\"] = torch.tensor([real_ged / higher_bound]).float().to(self.device)\n",
    "            new_data[\"ta_ged\"] = (torch.tensor(ta_ged).float() / higher_bound).to(self.device)\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        return new_data\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fitting a model.\n",
    "        \"\"\"\n",
    "        print(\"\\nModel training.\\n\")\n",
    "        t1 = time.time()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate, weight_decay=self.args.weight_decay)\n",
    "\n",
    "        self.model.train()\n",
    "        self.values = []\n",
    "        with tqdm(total=self.args.epochs * len(self.training_graphs), unit=\"graph_pairs\", leave=True, desc=\"Epoch\", file=sys.stdout) as pbar:\n",
    "            for epoch in range(self.args.epochs):\n",
    "                batches = self.create_batches()\n",
    "                loss_sum = 0\n",
    "                main_index = 0\n",
    "                for index, batch in enumerate(batches):\n",
    "                    batch_total_loss = self.process_batch(batch)  # without average\n",
    "                    loss_sum += batch_total_loss\n",
    "                    main_index += len(batch)\n",
    "                    loss = loss_sum / main_index  # the average loss of current epoch\n",
    "                    pbar.update(len(batch))\n",
    "                    pbar.set_description(\n",
    "                        \"Epoch_{}: loss={} - Batch_{}: loss={}\".format(self.cur_epoch + 1, round(1000 * loss, 3), index, round(1000 * batch_total_loss / len(batch), 3)))\n",
    "                tqdm.write(\"Epoch {}: loss={}\".format(self.cur_epoch + 1, round(1000 * loss, 3)))\n",
    "                training_loss = round(1000 * loss, 3)\n",
    "        t2 = time.time()\n",
    "        training_time = t2 - t1\n",
    "        if len(self.values) > 0:\n",
    "            self.prediction_analysis(self.values, \"training_score\")\n",
    "\n",
    "        self.results.append(\n",
    "            ('model_name', 'dataset', 'graph_set', \"current_epoch\", \"training_time(s/epoch)\", \"training_loss(1000x)\"))\n",
    "        self.results.append(\n",
    "            (self.args.model_name, self.args.dataset, \"train\", self.cur_epoch + 1, training_time, training_loss))\n",
    "\n",
    "        print(*self.results[-2], sep='\\t')\n",
    "        print(*self.results[-1], sep='\\t')\n",
    "        with open(self.args.abs_path + self.args.result_path + 'results.txt', 'a') as f:\n",
    "            print(\"## Training\", file=f)\n",
    "            print(\"```\", file=f)\n",
    "            print(*self.results[-2], sep='\\t', file=f)\n",
    "            print(*self.results[-1], sep='\\t', file=f)\n",
    "            print(\"```\\n\", file=f)\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_pk(num, pre, gt):\n",
    "        tmp = list(zip(gt, pre))\n",
    "        tmp.sort()\n",
    "        beta = []\n",
    "        for i, p in enumerate(tmp):\n",
    "            beta.append((p[1], p[0], i))\n",
    "        beta.sort()\n",
    "        ans = 0\n",
    "        for i in range(num):\n",
    "            if beta[i][2] < num:\n",
    "                ans += 1\n",
    "        return ans / num\n",
    "\n",
    "    def score(self, testing_graph_set='test', test_k=0):\n",
    "        \"\"\"\n",
    "        Scoring on the test set.\n",
    "        \"\"\"\n",
    "        print(\"\\n\\nModel evaluation on {} set.\\n\".format(testing_graph_set))\n",
    "        if testing_graph_set == 'test':\n",
    "            testing_graphs = self.testing_graphs\n",
    "        elif testing_graph_set == 'test2':\n",
    "            testing_graphs = self.testing2_graphs\n",
    "        elif testing_graph_set == 'val':\n",
    "            testing_graphs = self.val_graphs\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        self.model.eval()\n",
    "        # self.model.train()\n",
    "\n",
    "        num = 0  # total testing number\n",
    "        time_usage = []\n",
    "        mse = []  # score mse\n",
    "        mae = []  # ged mae\n",
    "        num_acc = 0  # the number of exact prediction (pre_ged == gt_ged)\n",
    "        num_fea = 0  # the number of feasible prediction (pre_ged >= gt_ged)\n",
    "        rho = []\n",
    "        tau = []\n",
    "        pk10 = []\n",
    "        pk20 = []\n",
    "\n",
    "        for pair_type, i, j_list in tqdm(testing_graphs, file=sys.stdout):\n",
    "            pre = []\n",
    "            gt = []\n",
    "            t1 = time.time()\n",
    "            for j in j_list:\n",
    "                data = self.pack_graph_pair((pair_type, i, j))\n",
    "                target, gt_ged = data[\"target\"].item(), data[\"ged\"]\n",
    "                model_out = self.model(data) if test_k == 0 else self.test_matching(data, test_k)\n",
    "                prediction, pre_ged = model_out[0], model_out[1]\n",
    "                if pre_ged == float('inf'):\n",
    "                    pre_ged = 999\n",
    "                round_pre_ged = round(pre_ged)\n",
    "\n",
    "                num += 1\n",
    "                if prediction is None:\n",
    "                    mse.append(-0.001)\n",
    "                elif prediction.shape[0] == 1:\n",
    "                    mse.append((prediction.item() - target) ** 2)\n",
    "                else:  # TaGSim\n",
    "                    mse.append(F.mse_loss(prediction, data[\"ta_ged\"]).item())\n",
    "                pre.append(pre_ged)\n",
    "                gt.append(gt_ged)\n",
    "\n",
    "                mae.append(abs(round_pre_ged - gt_ged))\n",
    "                if round_pre_ged == gt_ged:\n",
    "                    num_acc += 1\n",
    "                    num_fea += 1\n",
    "                elif round_pre_ged > gt_ged:\n",
    "                    num_fea += 1\n",
    "            t2 = time.time()\n",
    "            time_usage.append(t2 - t1)\n",
    "            rho.append(spearmanr(pre, gt)[0])\n",
    "            tau.append(kendalltau(pre, gt)[0])\n",
    "            pk10.append(self.cal_pk(10, pre, gt))\n",
    "            pk20.append(self.cal_pk(20, pre, gt))\n",
    "\n",
    "        time_usage = round(np.mean(time_usage), 3)\n",
    "        mse = round(np.mean(mse) * 1000, 3)\n",
    "        mae = round(np.mean(mae), 3)\n",
    "        acc = round(num_acc / num, 3)\n",
    "        fea = round(num_fea / num, 3)\n",
    "        rho = round(np.mean(rho), 3)\n",
    "        tau = round(np.mean(tau), 3)\n",
    "        pk10 = round(np.mean(pk10), 3)\n",
    "        pk20 = round(np.mean(pk20), 3)\n",
    "\n",
    "        self.results.append(('model_name', 'dataset', 'graph_set', '#testing_pairs', 'time_usage(s/100p)', 'mse', 'mae', 'acc', 'fea', 'rho', 'tau', 'pk10', 'pk20'))\n",
    "        self.results.append((self.args.model_name, self.args.dataset, testing_graph_set, num, time_usage, mse, mae, acc, fea, rho, tau, pk10, pk20))\n",
    "\n",
    "        print(*self.results[-2], sep='\\t')\n",
    "        print(*self.results[-1], sep='\\t')\n",
    "        with open(self.args.abs_path + self.args.result_path + 'results.txt', 'a') as f:\n",
    "            if test_k == 0:\n",
    "                print(\"## Testing\", file=f)\n",
    "            else:\n",
    "                print(\"## Post-processing\", file=f)\n",
    "            print(\"```\", file=f)\n",
    "            print(*self.results[-2], sep='\\t', file=f)\n",
    "            print(*self.results[-1], sep='\\t', file=f)\n",
    "            print(\"```\\n\", file=f)\n",
    "\n",
    "    def batch_score(self, testing_graph_set='test', test_k=100):\n",
    "        \"\"\"\n",
    "        Scoring on the test set.\n",
    "        \"\"\"\n",
    "        print(\"\\n\\nModel evaluation on {} set.\\n\".format(testing_graph_set))\n",
    "        if testing_graph_set == 'test':\n",
    "            testing_graphs = self.testing_graphs\n",
    "        elif testing_graph_set == 'test2':\n",
    "            testing_graphs = self.testing2_graphs\n",
    "        elif testing_graph_set == 'val':\n",
    "            testing_graphs = self.val_graphs\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        self.model.eval()\n",
    "        # self.model.train()\n",
    "\n",
    "        batch_results = []\n",
    "        for pair_type, i, j_list in tqdm(testing_graphs, file=sys.stdout):\n",
    "            res = []\n",
    "            for j in j_list:\n",
    "                data = self.pack_graph_pair((pair_type, i, j))\n",
    "                gt_ged = data[\"ged\"]\n",
    "                time_list, pre_ged_list = self.test_matching(data, test_k, batch_mode=True)\n",
    "                res.append((gt_ged, pre_ged_list, time_list))\n",
    "            batch_results.append(res)\n",
    "\n",
    "        batch_num = len(batch_results[0][0][1]) # len(pre_ged_list)\n",
    "        for i in range(batch_num):\n",
    "            time_usage = []\n",
    "            num = 0  # total testing number\n",
    "            mse = []  # score mse\n",
    "            mae = []  # ged mae\n",
    "            num_acc = 0  # the number of exact prediction (pre_ged == gt_ged)\n",
    "            num_fea = 0  # the number of feasible prediction (pre_ged >= gt_ged)\n",
    "            rho = []\n",
    "            tau = []\n",
    "            pk10 = []\n",
    "            pk20 = []\n",
    "\n",
    "            for res in batch_results:\n",
    "                pre = []\n",
    "                gt = []\n",
    "                for gt_ged, pre_ged_list, time_list in res:\n",
    "                    time_usage.append(time_list[i])\n",
    "                    pre_ged = pre_ged_list[i]\n",
    "                    round_pre_ged = round(pre_ged)\n",
    "\n",
    "                    num += 1\n",
    "                    mse.append(-0.001)\n",
    "                    pre.append(pre_ged)\n",
    "                    gt.append(gt_ged)\n",
    "\n",
    "                    mae.append(abs(round_pre_ged - gt_ged))\n",
    "                    if round_pre_ged == gt_ged:\n",
    "                        num_acc += 1\n",
    "                        num_fea += 1\n",
    "                    elif round_pre_ged > gt_ged:\n",
    "                        num_fea += 1\n",
    "                rho.append(spearmanr(pre, gt)[0])\n",
    "                tau.append(kendalltau(pre, gt)[0])\n",
    "                pk10.append(self.cal_pk(10, pre, gt))\n",
    "                pk20.append(self.cal_pk(20, pre, gt))\n",
    "\n",
    "            time_usage = round(np.mean(time_usage), 3)\n",
    "            mse = round(np.mean(mse) * 1000, 3)\n",
    "            mae = round(np.mean(mae), 3)\n",
    "            acc = round(num_acc / num, 3)\n",
    "            fea = round(num_fea / num, 3)\n",
    "            rho = round(np.mean(rho), 3)\n",
    "            tau = round(np.mean(tau), 3)\n",
    "            pk10 = round(np.mean(pk10), 3)\n",
    "            pk20 = round(np.mean(pk20), 3)\n",
    "            self.results.append((self.args.model_name, self.args.dataset, testing_graph_set, num, time_usage, mse, mae, acc, fea, rho, tau, pk10, pk20))\n",
    "\n",
    "            print(*self.results[-1], sep='\\t')\n",
    "            with open(self.args.abs_path + self.args.result_path + 'results.txt', 'a') as f:\n",
    "                print(*self.results[-1], sep='\\t', file=f)\n",
    "\n",
    "    def print_results(self):\n",
    "        for r in self.results:\n",
    "            print(*r, sep='\\t')\n",
    "\n",
    "        with open(self.args.abs_path + self.args.result_path + 'results.txt', 'a') as f:\n",
    "            for r in self.results:\n",
    "                print(*r, sep='\\t', file=f)\n",
    "\n",
    "    def test_matching(self, data, test_k, batch_mode=False):\n",
    "        prediction, pre_ged, soft_matrix = self.model(data)\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "        soft_matrix = (m(soft_matrix) * 1e9 + 1).round()\n",
    "        n1, n2 = soft_matrix.shape\n",
    "        # print(data[\"edge_index_1\"].shape)\n",
    "        g1 = dgl.graph((data[\"edge_index_1\"][0], data[\"edge_index_1\"][1]), num_nodes=n1)\n",
    "        g2 = dgl.graph((data[\"edge_index_2\"][0], data[\"edge_index_2\"][1]), num_nodes=n2)\n",
    "        g1.ndata['f'] = data[\"features_1\"]\n",
    "        g2.ndata['f'] = data[\"features_2\"]\n",
    "\n",
    "        if batch_mode:\n",
    "            t1 = time.time()\n",
    "            solver = KBestMSolver(soft_matrix, g1, g2)\n",
    "            res = []\n",
    "            time_usage = []\n",
    "            for i in [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "                if i > test_k:\n",
    "                    break\n",
    "                solver.get_matching(i)\n",
    "                min_res = solver.min_ged\n",
    "                t2 = time.time()\n",
    "                time_usage.append(t2 - t1)\n",
    "                res.append(min_res)\n",
    "                time_usage.append(t2 - t1)\n",
    "                res.append(min(pre_ged, min_res))\n",
    "            return time_usage, res\n",
    "        else:\n",
    "            solver = KBestMSolver(soft_matrix, g1, g2)\n",
    "            solver.get_matching(test_k)\n",
    "            min_res = solver.min_ged\n",
    "            return None, min_res\n",
    "\n",
    "    def prediction_analysis(self, values, info_str=''):\n",
    "        \"\"\"\n",
    "        Analyze the performance of value prediction.\n",
    "        :param values: an array of (pre_ged - gt_ged); Note that there is no abs function.\n",
    "        \"\"\"\n",
    "        if not self.args.prediction_analysis:\n",
    "            return\n",
    "        neg_num = 0\n",
    "        pos_num = 0\n",
    "        pos_error = 0.\n",
    "        neg_error = 0.\n",
    "        for v in values:\n",
    "            if v >= 0:\n",
    "                pos_num += 1\n",
    "                pos_error += v\n",
    "            else:\n",
    "                neg_num += 1\n",
    "                neg_error += v\n",
    "\n",
    "        tot_num = neg_num + pos_num\n",
    "        tot_error = pos_error - neg_error\n",
    "\n",
    "        pos_error = round(pos_error / pos_num, 3) if pos_num > 0 else None\n",
    "        neg_error = round(neg_error / neg_num, 3) if neg_num > 0 else None\n",
    "        tot_error = round(tot_error / tot_num, 3) if tot_num > 0 else None\n",
    "\n",
    "        with open(self.args.abs_path + self.args.result_path + self.args.dataset + '.txt', 'a') as f:\n",
    "            print(\"prediction_analysis\", info_str, sep='\\t', file=f)\n",
    "            print(\"num\", pos_num, neg_num, tot_num, sep='\\t', file=f)\n",
    "            print(\"err\", pos_error, neg_error, tot_error, sep='\\t', file=f)\n",
    "            print(\"--------------------\", file=f)\n",
    "\n",
    "    def demo_testing(self, testing_graph_set='test'):\n",
    "        print(\"\\n\\nDemo testing on {} set.\\n\".format(testing_graph_set))\n",
    "        self.testing_graph_set.append(testing_graph_set)\n",
    "        if testing_graph_set == 'test':\n",
    "            testing_graphs = self.testing_graphs\n",
    "        elif testing_graph_set == 'test2':\n",
    "            testing_graphs = self.testing2_graphs\n",
    "        elif testing_graph_set == 'val':\n",
    "            testing_graphs = self.val_graphs\n",
    "        elif testing_graph_set == 'train':\n",
    "            testing_graphs = self.training_graphs\n",
    "        else:\n",
    "            assert False\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # demo_num = 10\n",
    "        demo_num = len(testing_graphs)\n",
    "        # random.shuffle(testing_graphs)\n",
    "        testing_graphs = testing_graphs[:demo_num]\n",
    "        total_num = 0\n",
    "        num_10 = 0\n",
    "        num_100 = 0\n",
    "        num_1000 = 0\n",
    "        score_10 = [[], [], []]\n",
    "        score_100 = [[], [], []]\n",
    "        score_1000 = [[], [], []]\n",
    "\n",
    "        values0 = []\n",
    "        values1 = []\n",
    "        values2 = []\n",
    "        values3 = []\n",
    "\n",
    "        m = torch.nn.Softmax(dim=1)\n",
    "        for graph_pair in tqdm(testing_graphs, file=sys.stdout):\n",
    "            data = self.pack_graph_pair(graph_pair)\n",
    "            avg_v = data[\"avg_v\"]  # (n1+n2)/2.0, a scalar, not a tensor\n",
    "            gt_ged, target = data[\"ged\"], data[\"target\"]  # gt ged value and score\n",
    "            soft_matrix, _, prediction = self.model(data, is_testing=True)\n",
    "            pre_ged, gt_ged, gt_score = prediction.item(), gt_ged.item(), target.item()\n",
    "\n",
    "            values0.append(pre_ged - gt_ged)\n",
    "\n",
    "            soft_matrix = (torch.sigmoid(soft_matrix) * 1e9 + 1).round()\n",
    "            # soft_matrix = (m(soft_matrix) * 1e9 + 1).int()\n",
    "            # soft_matrix = ((soft_matrix - soft_matrix.min()) * 1e9 + 1).round()\n",
    "\n",
    "            n1, n2 = soft_matrix.shape\n",
    "            # print(data[\"edge_index_1\"].shape)\n",
    "            g1 = dgl.graph((data[\"edge_index_1\"][0], data[\"edge_index_1\"][1]), num_nodes=n1)\n",
    "            g2 = dgl.graph((data[\"edge_index_2\"][0], data[\"edge_index_2\"][1]), num_nodes=n2)\n",
    "            g1.ndata['f'] = data[\"features_1\"]\n",
    "            g2.ndata['f'] = data[\"features_2\"]\n",
    "\n",
    "            # if n1 < 10 or n2 < 10:\n",
    "            #   continue\n",
    "\n",
    "            total_num += 1\n",
    "            test_k = self.args.postk\n",
    "\n",
    "            solver = KBestMSolver(soft_matrix, g1, g2, pre_ged)\n",
    "            for k in range(test_k):\n",
    "                '''\n",
    "                matching, weightsum, sp_ged = solver.get_matching(k + 1)\n",
    "                if weightsum is None:\n",
    "                    print(k, solver.min_ged, gt_ged)\n",
    "                    break\n",
    "                mapping = torch.zeros([n1, n2])\n",
    "                for i, j in enumerate(matching):\n",
    "                    mapping[i][j] = 1.0\n",
    "                mapping_ged = self.model.ged_from_mapping(mapping, data[\"A_1\"], data[\"A_2\"], data[\"features_1\"], data[\"features_2\"])\n",
    "                min_res = min(min_res, mapping_ged.item())\n",
    "                '''\n",
    "                solver.get_matching(k + 1)\n",
    "                min_res = solver.min_ged\n",
    "                # a gt_mapping is found\n",
    "                if abs(min_res - gt_ged) < 1e-12:\n",
    "                    # fix pre_ged using lower bound\n",
    "                    fixed_pre_ged = max(solver.lb_value, pre_ged)\n",
    "                    # fix pre_ged using upper bound\n",
    "                    if min_res < fixed_pre_ged:\n",
    "                        fixed_pre_ged = min_res\n",
    "\n",
    "                    fixed_pre_s = exp(-fixed_pre_ged / avg_v)\n",
    "                    pre_score = abs(fixed_pre_ged - gt_ged)\n",
    "                    pre_score2 = (fixed_pre_s - gt_score) ** 2\n",
    "                    map_score = 0.0\n",
    "                    if k < 10:\n",
    "                        score_10[0].append(pre_score2)\n",
    "                        score_10[1].append(pre_score)\n",
    "                        score_10[2].append(map_score)\n",
    "                        num_10 += 1\n",
    "                        values1.append(fixed_pre_ged - gt_ged)\n",
    "                    if k < 100:\n",
    "                        score_100[0].append(pre_score2)\n",
    "                        score_100[1].append(pre_score)\n",
    "                        score_100[2].append(map_score)\n",
    "                        num_100 += 1\n",
    "                        values2.append(fixed_pre_ged - gt_ged)\n",
    "                    if k < 1000:\n",
    "                        score_1000[0].append(pre_score2)\n",
    "                        score_1000[1].append(pre_score)\n",
    "                        score_1000[2].append(map_score)\n",
    "                        num_1000 += 1\n",
    "                        values3.append(fixed_pre_ged - gt_ged)\n",
    "                    break\n",
    "                if k in [9, 99, 999]:\n",
    "                    # fix pre_ged using lower bound\n",
    "                    fixed_pre_ged = max(solver.lb_value, pre_ged)\n",
    "                    # fix pre_ged using upper bound\n",
    "                    if min_res < fixed_pre_ged:\n",
    "                        fixed_pre_ged = min_res\n",
    "\n",
    "                    fixed_pre_s = exp(-fixed_pre_ged / avg_v)\n",
    "                    pre_score = abs(fixed_pre_ged - gt_ged)\n",
    "                    pre_score2 = (fixed_pre_s - gt_score) ** 2\n",
    "                    map_score = abs(min_res - gt_ged)\n",
    "                    if k + 1 == 10:\n",
    "                        score_10[0].append(pre_score2)\n",
    "                        score_10[1].append(pre_score)\n",
    "                        score_10[2].append(map_score)\n",
    "                        values1.append(fixed_pre_ged - gt_ged)\n",
    "                    elif k + 1 == 100:\n",
    "                        score_100[0].append(pre_score2)\n",
    "                        score_100[1].append(pre_score)\n",
    "                        score_100[2].append(map_score)\n",
    "                        values2.append(fixed_pre_ged - gt_ged)\n",
    "                    elif k + 1 == 1000:\n",
    "                        score_1000[0].append(pre_score2)\n",
    "                        score_1000[1].append(pre_score)\n",
    "                        score_1000[2].append(map_score)\n",
    "                        values3.append(fixed_pre_ged - gt_ged)\n",
    "\n",
    "        if test_k >= 10:\n",
    "            print(\"10:\", len(score_10[0]), round(np.mean(score_10[1]), 3), round(np.mean(score_10[2]), 3), sep='\\t')\n",
    "            print(\"{} / {} = {}\".format(num_10, total_num, round(num_10 / total_num, 3)))\n",
    "        if test_k >= 100:\n",
    "            print(\"100:\", len(score_100[0]), round(np.mean(score_100[1]), 3), round(np.mean(score_100[2]), 3), sep='\\t')\n",
    "            print(\"{} / {} = {}\".format(num_100, total_num, round(num_100 / total_num, 3)))\n",
    "        if test_k >= 1000:\n",
    "            print(\"1000:\", len(score_1000[0]), round(np.mean(score_1000[1]), 3), round(np.mean(score_1000[2]), 3), sep='\\t')\n",
    "            print(\"{} / {} = {}\".format(num_1000, total_num, round(num_1000 / total_num, 3)))\n",
    "\n",
    "        with open(self.args.abs_path + self.args.result_path + self.args.dataset + '.txt', 'a') as f:\n",
    "            print('', file=f)\n",
    "            print(self.cur_epoch, testing_graph_set, demo_num, sep='\\t', file=f)\n",
    "            if test_k >= 10:\n",
    "                print(\"10\", round(np.mean(score_10[0]) * 1000, 3), round(np.mean(score_10[1]), 3), round(np.mean(score_10[2]), 3), round(num_10 / total_num, 3), sep='\\t', file=f)\n",
    "                # print(\"{} / {} = {}\".format(num_10, total_num, round(num_10 / total_num, 3)), file=f)\n",
    "            if test_k >= 100:\n",
    "                print(\"100\", round(np.mean(score_100[0]) * 1000, 3), round(np.mean(score_100[1]), 3), round(np.mean(score_100[2]), 3), round(num_100 / total_num, 3), sep='\\t', file=f)\n",
    "                # print(\"{} / {} = {}\".format(num_100, total_num, round(num_100 / total_num, 3)), file=f)\n",
    "            if test_k >= 1000:\n",
    "                print(\"1000\", round(np.mean(score_1000[0]) * 1000, 3), round(np.mean(score_1000[1]), 3), round(np.mean(score_1000[2]), 3), round(num_1000 / total_num, 3), sep='\\t', file=f)\n",
    "                # print(\"{} / {} = {}\".format(num_1000, total_num, round(num_1000 / total_num, 3)), file=f)\n",
    "            # print('', file=f)\n",
    "\n",
    "        self.prediction_analysis(values0, \"base\")\n",
    "        if test_k >= 10:\n",
    "            self.prediction_analysis(values1, \"10\")\n",
    "        if test_k >= 100:\n",
    "            self.prediction_analysis(values2, \"100\")\n",
    "        if test_k >= 1000:\n",
    "            self.prediction_analysis(values3, \"1000\")\n",
    "\n",
    "    def plot_error(self, errors, dataset=''):\n",
    "        name = self.args.dataset\n",
    "        if dataset:\n",
    "            name = name + '(' + dataset + ')'\n",
    "        plt.xlabel(\"Error\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.title(\"Error Distribution on {}\".format(name))\n",
    "\n",
    "        bins = list(range(int(max(errors)) + 2))\n",
    "        plt.hist(errors, bins=bins, density=True)\n",
    "        plt.savefig(self.args.abs_path + self.args.result_path + name + '_error.png', dpi=120,\n",
    "                    bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_error2d(self, errors, groundtruth, dataset=''):\n",
    "        name = self.args.dataset\n",
    "        if dataset:\n",
    "            name = name + '(' + dataset + ')'\n",
    "        plt.xlabel(\"Error\")\n",
    "        plt.ylabel(\"GroundTruth\")\n",
    "        plt.title(\"Error-GroundTruth Distribution on {}\".format(name))\n",
    "\n",
    "        # print(len(errors), len(groundtruth))\n",
    "        errors = [round(x) for x in errors]\n",
    "        groundtruth = [round(x) for x in groundtruth]\n",
    "        plt.hist2d(errors, groundtruth, density=True)\n",
    "        plt.colorbar()\n",
    "        plt.savefig(self.args.abs_path + self.args.result_path + '' + name + '_error2d.png', dpi=120,\n",
    "                    bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def plot_results(self):\n",
    "        results = torch.tensor(self.testing_results).t()\n",
    "        name = self.args.dataset\n",
    "        epoch = str(self.cur_epoch + 1)\n",
    "        n = results.shape[1]\n",
    "        x = torch.linspace(1, n, n)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(x, results[0], color=\"red\", linewidth=1, label='ground truth')\n",
    "        plt.plot(x, results[1], color=\"black\", linewidth=1, label='simgnn')\n",
    "        plt.plot(x, results[2], color=\"blue\", linewidth=1, label='matching')\n",
    "        plt.xlabel(\"test_pair\")\n",
    "        plt.ylabel(\"ged\")\n",
    "        plt.title(\"{} Epoch-{} Results\".format(name, epoch))\n",
    "        plt.legend()\n",
    "        # plt.ylim(-0.0,1.0)\n",
    "        plt.savefig(self.args.abs_path + self.args.result_path + name + '_' + epoch + '.png', dpi=120,\n",
    "                    bbox_inches='tight')\n",
    "        # plt.show()\n",
    "\n",
    "    def save(self, epoch):\n",
    "        torch.save(self.model.state_dict(), self.args.abs_path + self.args.model_path + self.args.dataset + '_' + str(epoch))\n",
    "\n",
    "    def load(self, epoch):\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(\n",
    "                self.args.abs_path + self.args.model_path + self.args.dataset + '_' + str(epoch), map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Getting params from the command line.\"\"\"\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parameter_parser(args : list):\n",
    "    \"\"\"\n",
    "    A method to parse up command line parameters.\n",
    "    The default hyperparameters give a high performance model without grid search.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Run GedGNN.\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"Number of training epochs. Default is 1.\")\n",
    "    parser.add_argument(\"--filters-1\", type=int, default=128, help=\"Filters (neurons) in 1st convolution. Default is 64.\")\n",
    "    parser.add_argument(\"--filters-2\", type=int, default=64, help=\"Filters (neurons) in 2nd convolution. Default is 32.\")\n",
    "    parser.add_argument(\"--filters-3\", type=int, default=32, help=\"Filters (neurons) in 3rd convolution. Default is 16.\")\n",
    "    parser.add_argument(\"--tensor-neurons\", type=int, default=16, help=\"Neurons in tensor network layer. Default is 16.\")\n",
    "    parser.add_argument(\"--bottle-neck-neurons\", type=int, default=16, help=\"Bottle neck layer neurons. Default is 16.\")\n",
    "    parser.add_argument(\"--bottle-neck-neurons-2\", type=int, default=8, help=\"2nd bottle neck layer neurons. Default is 8.\")\n",
    "    parser.add_argument(\"--bottle-neck-neurons-3\", type=int, default=4, help=\"3rd bottle neck layer neurons. Default is 4.\")\n",
    "    parser.add_argument(\"--bins\", type=int, default=16, help=\"Similarity score bins. Default is 16.\")\n",
    "    parser.add_argument(\"--hidden-dim\", type=int, default=16, help=\"the size of weight matrix in GedMatrixModule. Default is 16.\")\n",
    "    parser.add_argument(\"--histogram\", dest=\"histogram\", default=False, help='Whether to use histogram.')\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=128, help=\"Number of graph pairs per batch. Default is 128.\")\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.5, help=\"Dropout probability. Default is 0.5.\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.001, help=\"Learning rate. Default is 0.001.\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=5*10**-4, help=\"Adam weight decay. Default is 5*10^-4.\")\n",
    "    parser.add_argument(\"--demo\", dest=\"demo\", action=\"store_true\", default=False, help='Generate just a few graph pairs for training and testing.')\n",
    "    parser.add_argument(\"--gtmap\", dest=\"gtmap\", action=\"store_true\", default=False, help='Whether to pack gt mapping')\n",
    "    parser.add_argument(\"--value\", dest=\"value\", action=\"store_true\", default=False, help='Predict value. Otherwise predict mapping')\n",
    "    parser.add_argument(\"--finetune\", dest=\"finetune\", action=\"store_true\", default=False, help='Whether to use finetune.')\n",
    "    parser.add_argument(\"--prediction-analysis\", action=\"store_true\", default=False, help='Whether to analyze the bias of prediction.')\n",
    "    parser.add_argument(\"--postk\", type=int, default=1000, help=\"Find k-best matching in the post-processing algorithm. Default is 1000.\")\n",
    "    parser.add_argument(\"--abs-path\", type=str, default=\"\", help=\"the absolute path\")\n",
    "    parser.add_argument(\"--result-path\", type=str, default='result/', help=\"Where to save the evaluation results\")\n",
    "    parser.add_argument(\"--model-train\", type=int, default=1, help='Whether to train the model')\n",
    "    parser.add_argument(\"--model-path\", type=str, default='model_save/', help=\"Where to save the trained model\")\n",
    "    parser.add_argument(\"--model-epoch-start\", type=int, default=0, help=\"The number of epochs the initial saved model has been trained.\")\n",
    "    parser.add_argument(\"--model-epoch-end\", type=int, default=0, help=\"The number of epochs the final saved model has been trained.\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='AIDS', help=\"dataset name\")\n",
    "    parser.add_argument(\"--model-name\", type=str, default='GPN', help=\"model name\")\n",
    "    parser.add_argument(\"--graph-pair-mode\", type=str, default='combine', help=\"The way of generating graph pairs, including [normal, delta, combine].\")\n",
    "    parser.add_argument(\"--target-mode\", type=str, default='linear', help=\"The way of generating target, including [linear, exp].\")\n",
    "    parser.add_argument(\"--num-delta-graphs\", type=int, default=100, help=\"The number of synthetic delta graph pairs for each graph.\")\n",
    "    parser.add_argument(\"--num-testing-graphs\", type=int, default=100, help=\"The number of testing graph pairs for each graph.\")\n",
    "    parser.add_argument(\"--loss-weight\", type=float, default=1.0, help=\"In GedGNN, the weight of value loss. Default is 1.0.\")\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args : list):\n",
    "    \"\"\"\n",
    "    Parsing command line parameters, reading data.\n",
    "    Fitting and scoring a SimGNN model.\n",
    "    \"\"\"\n",
    "    args = parameter_parser(args)\n",
    "    tab_printer(args)\n",
    "\n",
    "    trainer = Trainer(args)\n",
    "    if args.model_epoch_start > 0:\n",
    "        trainer.load(args.model_epoch_start)\n",
    "\n",
    "    if args.model_train == 1:\n",
    "        for epoch in range(args.model_epoch_start, args.model_epoch_end):\n",
    "            trainer.cur_epoch = epoch\n",
    "            trainer.fit()\n",
    "            trainer.save(epoch + 1)\n",
    "            #trainer.score('val')\n",
    "            trainer.score('test')\n",
    "            #if not args.demo:\n",
    "            #   trainer.score('test2')\n",
    "    else:\n",
    "        trainer.cur_epoch = args.model_epoch_start\n",
    "        trainer.score('test', test_k=0)\n",
    "        # trainer.batch_score('test', test_k=100)\n",
    "        \"\"\"\n",
    "        test_matching = True\n",
    "        trainer.cur_epoch = args.model_epoch_start\n",
    "        #trainer.score('val', test_matching=test_matching)\n",
    "        trainer.score('test', test_matching=test_matching)\n",
    "        #if not args.demo:\n",
    "        #   trainer.score('test2')\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(args=[\n",
    "    \"--model-name=SimGNN\",\n",
    "    \"--dataset=Linux\",\n",
    "    \"--model-epoch-start=0\",\n",
    "    \"--model-epoch-end=1\",\n",
    "    \"--model-train=1\"\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
